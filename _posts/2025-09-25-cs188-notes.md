---
layout: post
title: "cs188学习笔记"
date: 2025-09-25
categories: 学习笔记
---

# 1 搜索问题
## 1.1 智能体（Agent）与环境（Environment）
**本节主要是术语(黑话)的解释，可以跳过。**

人工智能的目标：创造基于特定环境的智能体。

反射型智能体(reflex agent):根据当前环境做出行为，不考虑后果。
规划型智能体(planning agent):推断动作的后果并选择最优方案。
显然后者与人类的思考方式更一致，即前瞻性思考。

使用PEAS指标定义任务环境(性能指标performance measurement,环境 environment,执行器 actuators,传感器 sensors)。

环境类型有如下分别：
（不）可观察环境：智能体是否可以获取状态的完整信息。
（随机）确定环境：特定状态下执行动作是否得到唯一确定的结果。
（多）智能体环境：存在多个智能体，为避免被其他智能体预测，需要随机化动作。
（静）动态环境：环境是否随着智能体交互行为变化。
（不）可知环境：环境的物理规律是否已知。

## 1.2 状态空间与搜索问题
一个搜索问题需要有如下元素：
**状态空间（state space）**：所有可能的状态。
所有可能的**行动（actions）**。
**转移模型（transition model）**：根据当前状态输出下一状态。
**开销（cost）**：从一个状态转移到另一个的行动开销。
**初始状态（start state）**。
**目标检测（goal test）**：一个函数，检测输入某一状态，是否得到预期的输出。

以吃豆人游戏（Pacman）为例，我们假设现在只有棋盘（有障碍物），吃豆人和棋盘上的食物。我们可以设计出如下两种不同的搜索问题：
**寻路问题**
目标：找到从（$x_1,y_1$）到（$x_2,y_2$）的最优路径。
状态：吃豆人位置（$x,y$）。
行动：上下左右移动。
转移模型：更新位置。
目标检测：是否到达终点。
**吃豆问题**
目标：找到吃完所有食物的最优路径。
状态：吃豆人位置，每个食物是否被吃。
行动：上下左右移动。
转移模型：更新位置以及食物被吃状态。
目标检测：所有食物是否被吃。

可以看到对于以上两种问题，所维护的状态信息是不一样的。由此引出搜索状态（search state）和世界状态（world state）的不同。搜索状态只维护需要的信息，而世界状态会维护更多的信息。

### 1.2.1 状态空间尺度
一个搜索问题的时间开销往往取决于状态空间的大小，以吃豆人为例，我们估计状态空间的大小。

吃豆人游戏中有如下物体需要被考虑：
吃豆人位置，假设共有120个可选位置。
吃豆人朝向，有上下左右4个方向。
幽灵：有2个幽灵，假设每个幽灵可以处于12个状态。
食物：30个食物，每个有是否被吃2种状态。

因此状态空间的总尺度为：$120 \cdot 4\cdot 12^2 \cdot 2^{30}\approx 10^{13}$。

由此我们可以知道，只需把不同物体的状态数乘起来即可得到总的状态空间大小。

### 1.2.2 状态空间图和搜索树
**状态空间图**：
我们很容易想到可以构造一张图来描述整个状态空间，其中每个节点表示一个状态，每一条边表示一个行动。
然而这样的开销十分之大，完全存储所有内容显然是不可能的。

**搜索树**：
以初始状态为根节点，根据从当前节点可以执行的行动做出叶子节点，直到目标节点截止。
可以看出，每个状态并不只在搜索树中出现一次，因此存储下整个搜索树也是不可能的。

以上我们可以看出，完全存储下整个搜索树和状态空间图都是不现实的。在实际的搜索问题中，我们将只存储搜索树上的部分待考察节点，并且按照特定方法不断迭代到子节点上直至达到目标状态。下面将讲述迭代的不同方法。

## 1.3 无信息搜索（Uninformed Search）
首先介绍两个概念：局部计划（partial plan）和前沿（frontier）。
局部计划：从初始状态开始，执行了一些列动作后到达某个状态（不一定是目标状态）的路径。
前沿：一个存储了所有待探索的局部计划的重点的集合，表示当前搜索范围的边界。

我们按照如下标准方式进行从初始状态到目标状态的搜索：
维护一个前沿，每一次根据特定策略移除其中一个节点，并将其所有的子节点替换到前沿中。直到从前沿中移除一个目标状态，则其所对应的局部计划即为从起始状态到目标状态的路径。

评判一个策略，有如下几个方面需要考虑：
完备性：搜索问题如果存在解，是否一定能被找到（不计开销）。
最优性：是否能保证找到最低成本路径。
分支因子：每次将前沿中的一个节点替换为子节点时，前沿节点数量增长率O(b)。对深度为k的搜索树，有$O(b^k)$个节点。
搜索树最大深度：m。
最浅解所在深度：s。

如果对于目标状态在搜索树中的位置一无所知，称为无信息搜索。对于此类搜索，有如下三种策略：深度优先、广度优先、一致代价。每一种策略对应的是不同的前沿扩展方式。

### 1.3.1 深度优先搜索（DFS）策略
每次取出当前的前沿中深度最大的一个节点，并用子节点代替它。因此前沿是一个后入先出（LIFO）的数据结构，用栈（stack）来存储。
完备性：无法保证，如果有环的存在，则会被卡住。
最优性：无法保证，只能搜出最左下的解。
时间复杂度：最差情况下搜遍整棵树$O(b^m)$。
空间复杂度：最差情况下每一层都要存b个节点O(bm)。
典型的代码如下所示：
```python
frontier = util.Stack()
visited = set() # 避免重复访问同一节点
node = {"state" : problem.getStartState() , "path" : []}
# 每个节点维护2个信息，分别是状态和路径
frontier.push(node)

while True:
    if frontier.isEmpty():
        return None
        
    node = frontier.pop()
    if problem.isGoalState(node["state"]):
        return node["path"]
        
    if node["state"] not in visited:
        visited.add(node["state"])
        for child in problem.getSuccessors(node["state"]):
            # getSuccessors()方法得到一个长度为2的列表，
            # 分别是新的状态，状态转移对应的行动
            child_node = {"state" : child[0] , "path" : node["path"] + [child[1]]}
            frontier.push(child_node)
```

### 1.3.2 广度优先搜索（BFS）策略
每次取出当前的前沿中深度最小的一个节点，并用子节点代替它。因此前沿是一个先入先出（FIFO）的数据结构，用队列（queue）来存储。
完备性：满足。
最优性：仅当所有边权重相同时满足，总是找到最浅的解。
时间复杂度：最差需要搜索$1+b+b^2+\dots +b^s$个节点，时间复杂度$O(b^s)$。
空间复杂度：最差需要存下整棵树$O(b^s)$。
典型的代码如下所示：
```python
frontier = util.Stack()
visited = set() # 避免重复访问同一节点
node = {"state" : problem.getStartState() , "path" : []}
# 每个节点维护2个信息，分别是状态和路径
frontier.push(node)

while True:
    if frontier.isEmpty():
        return None
        
    node = frontier.pop()
    if problem.isGoalState(node["state"]):
        return node["path"]
        
    if node["state"] not in visited:
        visited.add(node["state"])
        for child in problem.getSuccessors(node["state"]):
            # getSuccessors()方法得到一个长度为2的列表，
            # 分别是新的状态，状态转移对应的行动
            child_node = {"state" : child[0] , "path" : node["path"] + [child[1]]}
            frontier.push(child_node)
```

### 1.3.3 一致代价搜索（UCS）策略
每次扩展前沿中开销最低的局部路径的终点。因此前沿是一个基于堆的优先队列（priority queue）。每一次移除操作并替换其子节点时，会重新调整自身顺序以维持按照路径开销的升序排序。
完备性：满足。
最优性：只要边权非负，则满足最优性。类似于最短路Dijkstra算法，只不过我们并不要求找到通往所有状态的最短路径，而是只需要特定目标状态的最短路径。
时间复杂度：最优总开销$C^* $，每一步最小开销（最小边权）$\epsilon$，则最差情况下搜遍$\frac{C^*}{\epsilon}$层，时间复杂度$O(b^{\frac{C^ *}{\epsilon}})$。
空间复杂度：存下整个树$O(b^{\frac{C^ *}{\epsilon}})$。
典型的代码如下所示：
```python
frontier = util.PriorityQueue()
visited = set() # 避免重复访问同一节点
node = {"state" : problem.getStartState() , "path" : [] , "cost" : 0}
# 每个节点维护3个信息，分别是状态、路径和开销
frontier.push(node , 0)

while True:
    if frontier.isEmpty():
        return None
        
    node = frontier.pop()
    if problem.isGoalState(node["state"]):
        return node["path"]
        
    if node["state"] not in visited:
        visited.add(node["state"])
        for child in problem.getSuccessors(node["state"]):
            # getSuccessors()方法得到一个长度为3的列表，
            # 分别是新的状态，状态转移对应的行动，状态转移对应的行动开销
            child_node = {"state" : child[0] , "path" : node["path"] + [child[1]] , "cost" : node["cost"] + child[2]}
            frontier.update(child_node , child_node["cost"]) # 注意需要更新
```

## 1.4 有信息搜索（Informed Search）
虽然UCS策略满足完备性和最优性，但它的开销仍然是高昂的，因为我们总是在漫无目的地搜索。如果我们能够预先知道搜索的方向，则整个过程将变得更快捷。这正是所谓的有信息搜索（informed search）。

### 1.4.1 启发式搜索（Heuristics）
启发式函数给出当前状态到目标状态之间的距离的一种估计。通常而言我们需要使得给出的估计值小于真实距离（原因见后）。以吃豆人为例，我们可以将该启发式函数设置为当前位置到目标位置之间的曼哈顿距离（$|x_1-x_2|+|y_1-y_2|$）。显然该过程去掉了许多约束（墙的阻挡），因此启发式搜索通常考虑的是原问题的松弛问题（relaxed problems）。

启发式搜索有点类似于告诉计算机一种直觉，让它始终跟着该直觉进行选择下一步行动。

根据启发式函数，我们有两种搜索方案：贪心搜索和$A^*$算法。

### 1.4.2 贪心搜索（Greedy Search）
每次选取启发式函数值最短的节点进行扩展前沿。与UCS的区别仅仅在于，UCS的优先指标是从初始状态到当前节点的路径开销，而贪心搜索的优先指标是从当前节点到目标节点的启发式函数给出的距离估计。

完备性与最优性：显然启发式函数并不能保证一定探索到目标，更不能保证最优性。如果启发式函数选择的不好的话，这个策略将表现得极不可靠，可能直接搜索到目标，也可能始终在探索错误区域。

### 1.4.3 $A^*$搜索
与贪心搜索和UCS类似，我们仍用优先队列维护前沿，只不过优先指标是前两者的和，即从初始状态到当前节点的路径开销与当前状态到目标状态的启发式函数给出的估计距离的和，可以理解为这是经过该节点的合理路径的总开销估计值。每一次我们扩展的是总估计开销最小的节点。

完备性与最优性：如果选择合适的启发式函数，这两点均将得到保证（之后将详细说明）。由此，$A^*$搜索综合了UCS的完备性和最优性和贪心搜索的高效性，是目前为止最优的策略。

### 1.4.4 可采纳性（Admissibility）
对于节点n，UCS、贪心搜索和$A^*$搜索维护的优先队列的函数分别是：$g(n),h(n),f(n)=g(n)+h(n)$。我们需要知道选取什么样的启发式函数$h(n)$更好。

首先我们需要说明，并非任何启发式函数都可以保证$A^* $搜索的完备性和最优性。如果$h(n)=1-g(n)$，则$f(n)=1$，$A^*$搜索退化为BFS，对于边权不同的情形，并不能保证最优性。

为了使$A^* $搜索满足完备性和最优性，启发函数需要满足如下可采纳性条件：$\forall n,0 \leq h(n) \leq h^* (n)$，其中$h^* (n)$为从当前状态到目标状态的最小真实代价。

**定理**：如果可采纳性满足，则$A^* $搜索一定能够得到最优解。

**证明**：最优目标状态A，次优状态B。A的祖先n在当前前沿中，则为了保证最优性，n必须先于B被扩展。
$g(A)<g(B)$，因为A是最优解；
$h(A)=h(B)=0$，因为二者都是目标状态，有$h^* (A)=h^* (B)=0$；
于是有$f(A)=g(A) < g(B)=f(B)$。
同时$f(n)\leq f(A)$，因为$f(n)=g(n)+h(n) \leq g(n)+h^* (n) = g(A)=f(A)$。
于是有$f(n)\leq f(A) < f(B) \Rightarrow f(n) < f(B)$。

对于$A^* $搜索，有时我们会反复搜索一个节点导致陷入死循环或者浪费大量寻找时间。一种简单的处理方式是维护一个已到达状态的集合，确保每次只扩展未到达的集合，这种方式称为图搜索（graph search）。然而这样会导致可能无法搜到最优解，考虑如下情况：
![alt text](/assets/images/image.png)
如果不加入已到达集合的判断，我们将会按照以下步骤扩展：
$S\to A , B ; B\to C ; A \to C ; C \to G$。
即我们会先扩展A节点覆盖原先的C节点，以保证得到最优解。
然而如果加入已到达集合的判断，将无法覆盖原先的C节点，导致无法搜出最优解。因此我们还需要加入额外的判断条件，即对于二次到达的节点，是否找到了一条更优的路径。

我们可以使用一个字典（注意不是列表，否则访问开销大）来存储已到达节点及其开销，在扩展新节点时，总是检查其是否未被访问或者当前路径开销小于原开销，若否，则不进行节点扩展。下面给出伪代码：
```
function A*-graph_search(problem , frontier) [return solution or failure]
    visited # 字典，key是node，value是cost，初始为空
    frontier # 优先队列，初始为空
    insert(make_node(initial_state(problem)) , frontier)
    while not is_empty(frontier) do
        node <- pop(frontier)
        if problem.is_goal(node.state) return node
        if (node.state is not in visited) or (visited[node.state] > node.cost)
            visited[node.state] = node.cost
            for child_node in expand(problem , state) do
                insert((child_node , child_node.cost + node.cost) , frontier)
    return failure
```
典型代码如下所示：
```python
frontier = util.PriorityQueue()
visited = {} # 避免重复访问同一节点
node = {"state" : problem.getStartState() , "path" : [] , "cost" : 0}
# 每个节点维护3个信息，分别是状态、路径和开销
frontier.push(node , node["cost"] + heuristic(node["state"] , problem))

while True:
    if frontier.isEmpty():
        return None
        
    node = frontier.pop()
    if problem.isGoalState(node["state"]):
        return node["path"]
        
    if node["state"] not in visited or node["cost"] < visited[node["state"]]:
        visited[node["state"]] = node["cost"]
        for child in problem.getSuccessors(node["state"]):
            # getSuccessors()方法得到一个长度为3的列表，
            # 分别是新的状态，状态转移对应的行动，状态转移对应的行动开销
            child_node = {"state" : child[0] , "path" : node["path"] + [child[1]] , "cost" : node["cost"] + child[2]}
            frontier.update(child_node , child_node["cost"] + heuristic(child_node["state"] , problem)) # 注意需要更新
```

### 1.4.5 主导性（Dominance）
现在考虑如何评价两个启发函数中哪一个更好。显然，越接近于真实的开销的启发函数是越好的，启发函数为零则退化为UCS.因此，如果有$\forall n : h_a(n) \geq h_b(n)$，则说明启发函数a优于启发函数b。

对于多个满足可采纳性条件的启发函数，其最大值函数总是满足可采纳性条件，并且该极大值函数优于各个启发函数。因此我们总是可以通过计算多个可采纳的启发函数的最大值函数的方式得到一个更优的启发函数。

# 2 约束满足问题（Constraint Satisfaction Problems ， CSPs）
## 2.1 约束问题与约束图
搜索问题希望得到的结果是从初始状态到达目标状态的最小开销的路径，而约束满足问题则是希望得到满足约束条件的目标状态。
我们可以用更抽象严谨的方法表述CSPs，采用如下三个要素定义：变量、定义域、约束条件。
以8皇后问题为例：
变量$X_{ij}$记录棋盘上每个位置是否放置皇后；
每个$X_{ij}$的定义域是{0 , 1};
约束条件如下：
每行只有一个皇后 -- $\forall i,j,k (X_{ij} , X_{ik} ) \in \{ （0 , 0） ， （0 , 1） ， （1 , 0） \}$；
每列只有一个皇后 -- $\forall i,j,k (X_{ij} , X_{kj} ) \in \{ （0 , 0） ， （0 , 1） ， （1 , 0） \}$；
每条斜线上只有一个皇后 -- 
$\forall i,j,k (X_{ij} , X_{i + k , j + k} ) \in \{ （0 , 0） ， （0 , 1） ， （1 , 0） \}$；
$\forall i,j,k (X_{ij} , X_{i + k , j - k} ) \in \{ （0 , 0） ， （0 , 1） ， （1 , 0） \}$；

约束问题是NP-hard问题，不存在多项式时间内的解法。通常转化为搜索问题求解，中间状态即为一部分变量赋值，另一部分变量未赋值的状态，其后继即为新增一个赋值变量。本节希望利用合理的启发方法在可接受时间内解决该问题。

我们可以用约束图来表述约束问题，其中顶点表示变量，边表示最常规的两个变量之间的约束。当然有些约束仅作用于单个变量上，有些约束需要作用在多个变量上，这会使得约束图有些不同。
考虑一个典型的CSP：地图着色问题。给定地图和若干颜色，希望得到相邻区域着色不同的着色方案。这个问题的约束总是二元约束（对于单独的区域，并不带来约束），因此可以容易地给出如下约束图（顶点中的字母代表不同区域的名称）：
![alt text](/assets/images/image-1.png)
将约束问题抽象成约束图，可以清晰地看出其中的结构特征（稀疏，树状等），以便深入研究。

## 2.2 回溯搜索
对CSP，我们常用的方法是回溯搜索。实际上就是在DFS的基础上增加了判断条件（剪枝），对于每一次的探索，仅考虑与已赋值变量之间满足约束条件的那些扩展，若不存在则不继续扩展，直接回溯。这相当于在扩展过程中始终保持约束条件满足，而非最后统一判断，减少了直接深度搜索中不少错误尝试。
![alt text](/assets/images/image-2.png)
在此基础上，还有诸多可以优化之处，下面注意解释：

### 2.2.1 剪枝优化
最简单的剪枝即向前检查。就是每次新赋值变量时执行一次检查，在约束图中修剪新赋值变量附近的未赋值变量的定义域。

向前检查的思路简单，但是并不排除掉所有能用逻辑排除的待探索方案。以下图为例，假设只能染红、绿、蓝三种颜色：
![alt text](/assets/images/image-3.png)
由于左右与红、绿相邻，因此NT,SA均只能染蓝色；而二者本身相邻，不能同时染蓝色，因此当前状态其实不必继续探索了。想要用程序实现这样的逻辑推理，需使用弧一致性算法（arc consistency）。

在向前检查的基础上，我们可以得到未赋值变量被修剪后的定义域。我们要做的即检查这之间是否存在矛盾。
对于约束图中每条边（只需考虑未赋值变量之间的），可以看成是两条反向的单向边，我们将其存入队列Q中，我们将依次检查队列中的每条边。
检查边的逻辑如下。对每条tail$\to$head的边，需要检查tail变量的定义域中的每一个，是否总是有head变量的定义域中的值与之匹配。若总是匹配，则检查结束，移除该单向边；若否，则修改tail变量的定义域，并且将所有指向tail的边（next$\to$tail）放进队列中（这是因为这些约束可能需要被去掉的变量值才能达成，所以需重新检查），然后移除原单向边。
如果过程中出现定义域为空的情况，说明当前状态无合理解，直接终止检查。

典型的算法称之为AC-3算法，伪代码如下：
```
function AC-3:
    queue
    while queue is not empty
        (tail , head) = queue.pop()
        if inconsistent(tail , head):
            for k in neighbors[tail]:
                queue.add((k , tail))

function inconsistent(tail , head):
    flag = False
    for x in domain[tail]:
        if no y in domain[head] satisfy(x , y)
            delete x from domain[tail]
            flag = true
    return flag
```
弧一致性算法的时间复杂度是$O(ed^3)$，d是最大定义域大小，e是有向边数量。弧一致性算法能剪枝更彻底，减少后续回溯，但是需要更多的时间来进行判断，因此需要权衡选择。

弧一致性可以推广为k一致性（k-consistency），它指的是对于任何k个节点，对其中任意k-1个节点赋值（定义域中的），总能保证第k个节点至少有一个合理值。弧一致性即等价于2一致性。

### 2.2.2 排序优化（Ordering）
从最深处出发，往往有多个节点可供探索，优先探索哪一个节点存在排序问题。在CSP中，这种排序对应到两处选择：
（1）优先赋值哪个变量；
（2）优先赋值定义域中哪个变量值。
通常根据两个准则来处理以上两处的选择问题：
（1）最少剩余值（MRV），优先赋值定义域最小，约束最紧的变量，因为这个方向最有可能触发回溯；
（2）最少约束值（LCV），优先赋的变量值是那个能从剩余未赋值变量的定义域中修剪掉最少的变量值的那个。选择最少其实很好理解，因为如果用逻辑能够排除的情况最少，那么枚举这个点的收益应该是最高的。这需要为每个变量值运行一次剪枝算法（向前检查或弧一致性），会消耗额外的时间，但可以提升总体速度。

### 2.2.3 结构优化
对于无环的约束图，实际上仅靠一致性即可得到满足约束条件的答案。这种做法称为树结构CSP算法，具体如下：
首先选定根节点，实际上随便选即可，无环图的任意节点均可以作为树的根节点；
第二步是进行拓扑排序，使得约束条件的方向从左至右。由于无环，我们可以把约束全部当成单向边，即我们从根节点做了一步枚举之后，并不会存在后续填入反过来约束根节点的情况。具体如图所示：
![alt text](/assets/images/image-4.png)
第三步是执行反向弧一致性检查。由于无环，我们只需要从右至左依次检查即可，也不需要在删除点之后重新入边，因为这些可以重新入的边必然是上一层的，已经处于待检查队列中。
最后我们从根节点开始填入合适值即可。由于已经执行了一致性检查，因此对于每一个可供选择的变量值，后续节点总是存在解。树结构CSP的时间复杂度仅为$O(nd^2)$,n为节点数,d为最多可选择的变量值数目，远小于$O(d^n)$。

对于不是树结构的约束图，我们总是可以通过去掉部分节点使其成为树。去掉最小节点的方案称为最小割集，我们只需枚举割集的状态，并根据割集状态得到剩余树状约束图的初始定义域，即可用之前的树结构CSP算法求解。具体示例如下：
![alt text](/assets/images/image-5.png)
割掉的部分大小为c，因此需要独立枚举$O(d^c)$种着色情况，总的时间复杂度为$O(d^c(n-c)d^2)$，对于较小的c，该方法十分高效。

## 2.3 局部搜索（Local Search）
回溯搜索是在按照一定逻辑一个一个填写变量值，我们也可以通过局部搜索方法求解CSP。我们可以定义搜索问题，此时状态空间是一个对结果的猜测（可能不满足条件或者并非最优）。以数独问题为例，状态就是填满数字的表格（不一定符合要求）。我们只需要向尽可能满足约束的方向进行搜索即可。
对于此类情形，实际上类似于对一个自变量范围（状态空间）很大的目标函数，我们希望求得全局最大（小）值。但是由于状态空间过大，我们每次能搜索的只是自变量上的一小段区间，因此称之为局部搜索。我们正是希望利用局部的搜索来求得全局最值，下面介绍四种方法：爬山算法，模拟退火，局部束搜索和遗传算法。

### 2.3.1 爬山算法（Hill-Climbing Search）
这个方法原理极其简单，就是每次移动到当前可搜索区间内的最大值。显然这是不完备的，很容易被卡在局部极大值上（只要局部极值的管辖范围超过搜索区间）。可以加入随机数优化搜索的起点，理论上只要随机次数够多，总能找到合适的能捕捉到全局最大值的起点。

### 2.3.2 模拟退火（Simulated Annealing）
爬山算法被卡住的原因在于不允许向目标函数减小的方向搜索，可以针对这点加以改进。对于每一次随机的更新，如果目标函数提高则直接接受；如果减小，则以一定概率接受向减小的方向移动。这个概率受“温度”调控，初始时温度较高，允许更多的不好的移动，而后温度逐渐降低。理论上，如果温度下降足够缓慢，则模拟退火以接近1的概率达到全局最大值。

### 2.3.3 局部束搜索（Local Beam Search）
k个线程的爬山算法，但它们并不是各自独立进行的，否则和多跑几遍爬山算法没区别。选择新的k个线程，是在原先k个线程的可搜索范围的总体上选择最大的k个，因此比较差的原始选择会被吸引到较好的选择附近。当然，对于平坦的局域极大值，这种做法仍会被卡住，因此还是需要引入随机初始位置进行规避。

### 2.3.4 遗传算法（Genetic Algorithm）
顾名思义，这个算法受生物进化启发，具体有如下操作：
我们有一个种群，其中每个个体代表了一种状态，现在我们要对种群进行更新。
对于当前种群中每个个体状态，我们可以打分，并按照分数比例生成抽样的概率，总概率是归一的。
一个新个体的生成是这样的：按照上述概率抽出两个个体，并进行杂交。所谓杂交指的是随机选择一个位置，该位置的前后两半分别继承自抽出的两个个体。并按照一定的小概率，执行基因突变，即直接修改某个位置的值。
每次生成的新个体数目维持种群总个体数不变，维持足够多轮次之后，选出其中最优的那一个个体作为最终答案。
遗传算法的优势在于，有足够多的交换使得更有潜力获得更高的总分。

其实通过算法的解释我们可以知道，局部搜索并不能够保证能找到最优解，但是对于相当多的情况都有效。这里存在一个关键的比例$R=\frac{条件数}{变量数}$，一旦超过某个临界值，局部搜索的开销就会十分昂贵。

# 3 博弈问题（Games）
## 3.1 博弈问题的定义
之前我们解决的都是找最佳解或者合法解的问题，但有些时候，环境状态并不是确定的，而是多个智能体在进行博弈。这时我们需要智能体学会预测对手的行为并制定计划，即对抗性搜索方法，也即博弈。
最常见的博弈是零和博弈，一方得分另一方就失分，一个智能体试图最大化得分，另一个则最小化分数。在吃豆人中，玩家通过吃豆最大化分数，而幽灵通过吃吃玩家来最大化分数。对抗性程序每一步返回的不是完整的路径，而是根据当前环境配置（自己与对手）得出的最佳行动方案。
标准博弈模型由以下要素构成：
初始状态，$s_0$；
玩家，$Players(s)$，表示当前行动方；
行动，$Actions(s)$，表示当前玩家可执行的行动；
转移模型，$Result(s , a)$，表示行动后状态的更新;
终止检测，$Terminal_test(s)$，判断游戏是否结束；
终止效益，$Utility(s,player)$，评估游戏按该状态结束的得分。

## 3.2 零和博弈
### 3.2.1 极小极大算法（Minimax）
零和博弈的第一个典型算法是Minimax，它的预设是对手行为总是最优的。在介绍算法前，先解释状态价值（state value）和终止效益（terminal utility）。构建博弈树，并设定评分规则（初始10分，走一步扣1分，吃到豆子为止）：
![alt text](/assets/images/image-6.png)
终止效益指的是某个给定的终止节点的得分；
状态价值指的是其后代中终止节点得分的最大值。
显然，生成状态价值的方式是从底部逐层向上返回最大值。

有对抗的博弈树如下所示，蓝色代表吃豆人控制，红色代表幽灵控制：
![alt text](/assets/images/image-7.png)
与之前不同，由于幽灵的控制，吃豆人并不能够理想地走到所有子节点中终止效益最大的终点。它会假设幽灵会选择使终止效益最小的走法，因而自己只能在其基础上再选择终止效益最大的走法。这就是所谓的Minimax算法。
实际上实现很简单，就是一个后序遍历博弈树。从下至上生成每个节点的状态价值使，根据当前由吃豆人还是玩家控制，分别选择子节点状态价值的最大或最小值。

典型的Minimax算法代码如下所示：
```python
class MinimaxAgent(MultiAgentSearchAgent):
    def getValue(self , gameState : GameState , agentIndex , depth):
        # 获取状态价值函数
        legalActions = gameState.getLegalActions(agentIndex)
        if len(legalActions) == 0:
            return self.evaluationFunction(gameState)
        
        if agentIndex == 0:
            # 更新当前探索深度
            depth += 1
            # 终止节点条件，返回估值函数
            if depth == self.depth:
                return self.evaluationFunction(gameState)
            else:
                return self.max_value(gameState , agentIndex , depth)
        elif agentIndex > 0:
            return self.min_value(gameState , agentIndex , depth)
    
    def max_value(self , gameState : GameState , agentIndex , depth):
        maxx = -1e9
        legalActions = gameState.getLegalActions(agentIndex)
        for action in legalActions:
            value = self.getValue(gameState.generateSuccessor(agentIndex , action) , (agentIndex + 1) % gameState.getNumAgents() , depth)
            if value is not None and value > maxx:
                maxx = value
        return maxx
    
    def min_value(self , gameState : GameState , agentIndex , depth):
        minn = 1e9
        legalActions = gameState.getLegalActions(agentIndex)
        for action in legalActions:
            value = self.getValue(gameState.generateSuccessor(agentIndex , action) , (agentIndex + 1) % gameState.getNumAgents() , depth)
            if value is not None and value < minn:
                minn = value
        return minn
    
    def getAction(self, gameState: GameState):
        # 根节点的行动选择，相当于整个选择的驱动
        maxx = - 1e9
        best_action = None
        for action in gameState.getLegalActions(agentIndex=0):
            value = self.getValue(gameState.generateSuccessor(agentIndex=0 , action=action) , agentIndex=1 , depth=0)
            if value is not None and value > maxx:
                maxx = value
                best_action = action
        return best_action
```

#### 3.2.1.1 Alpha-Beta 剪枝
Minimax的想法简单而直接，但是想要搜出结果的时间复杂度是不可接受的。其过程类似于DFS，时间复杂$O(b^m)$，b是分支数，m是树的深度。因此我们需要进行优化——Alpha-Beta剪枝。
我们通过以下例子来感受该剪枝方法，方框表示终止节点，下三角表示最小值节点，上三角表示最大值节点。
![alt text](/assets/images/image-8.png)
我们现在在确定根节点的值。它会先遍历左节点，然后继续向下，遍历完后返回左节点最小值3，并更新当前最大值3；然后遍历中间节点，当向下遍历到终止节点2时，我们就不需要再遍历其它终止节点了，因为此时中间节点最大为2，不可能大于我们已有的3；再遍历右节点，需要全部遍历完。总的遍历范围如下所示：
![alt text](/assets/images/image-9.png)
这种剪枝带来的效果理论最佳情况下可以使搜索深度翻倍，然而实际情况达不到，但仍能使搜索深度增加一两层。
更确切地，我们这么解释该算法：
维护alpha（目前路径Max得分下限，就是目前最大）和beta(目前路径Min得分上限，就是目前最小)；
假设我们搜索到某一个Min节点，它的父节点是一个Max节点，我们需要先搜索自己的子节点找到最小再传给父节点。但是我们搜索子节点时如果已经找到比alpha更小的点，父节点就根本不会考虑当前这个节点了，因此也无需搜索当前节点的剩余子节点了。
对于Max节点同理，如果已经搜到子节点大于父节点可选的beta值，也无需进行剩余子节点的搜索了。

典型的alpha-beta剪枝代码如下所示：
```python
class AlphaBetaAgent(MultiAgentSearchAgent):
    def getValue(self , gameState : GameState , agentIndex , depth , alpha , beta):
        # 获取状态价值函数
        legalActions = gameState.getLegalActions(agentIndex)
        if len(legalActions) == 0:
            return self.evaluationFunction(gameState)
        
        if agentIndex == 0:
            # 更新当前探索深度
            depth += 1
            # 终止节点条件，返回估值函数
            if depth == self.depth:
                return self.evaluationFunction(gameState)
            else:
                return self.max_value(gameState , agentIndex , depth , alpha , beta)
        elif agentIndex > 0:
            return self.min_value(gameState , agentIndex , depth , alpha , beta)
    
    def max_value(self , gameState : GameState , agentIndex , depth , alpha , beta):
        # 当前是最大值节点，从父节点继承beta，只要子节点大于beta则无需继续搜索其他子节点
        maxx = -1e9
        legalActions = gameState.getLegalActions(agentIndex)
        for action in legalActions:
            value = self.getValue(gameState.generateSuccessor(agentIndex , action) , (agentIndex + 1) % gameState.getNumAgents() , depth , alpha , beta)
            if value is not None and value > maxx:
                maxx = value
            if maxx > beta:
                return maxx
            alpha = max(alpha , maxx) # 更新alpha
        return maxx
    
    def min_value(self , gameState : GameState , agentIndex , depth , alpha , beta):
        # 当前是最小值节点，从父节点继承alpha，只要子节点小于alpha则无需继续搜索其他子节点
        minn = 1e9
        legalActions = gameState.getLegalActions(agentIndex)
        for action in legalActions:
            value = self.getValue(gameState.generateSuccessor(agentIndex , action) , (agentIndex + 1) % gameState.getNumAgents() , depth , alpha , beta)
            if value is not None and value < minn:
                minn = value
            if minn < alpha:
                return minn
            beta = min(beta , minn) # 更新beta
        return minn
    
    def getAction(self, gameState: GameState):
        # 根节点的行动选择
        maxx = - 1e9
        alpha = -1e9
        best_action = None
        for action in gameState.getLegalActions(agentIndex=0):
            value = self.getValue(gameState.generateSuccessor(agentIndex=0 , action=action) , agentIndex=1 , depth=0 , alpha=alpha , beta=1e9)
            if value is not None and value > maxx:
                maxx = value
                best_action = action
            alpha = max(alpha , maxx) # 更新alpha
        return best_action
```

#### 3.2.1.2 估值函数（Evaluation Functions）
虽然有alpha-beta剪枝，我们大概率仍然无法搜索到树的终止节点，这时候就需要给出一个状态价值的估计值。我们每次搜索到最深处之后，直接调用估值函数得到状态价值的估计。显然，这么做无法保证总是取得最优表现。另外需要说明的是，在启用估值函数之前搜索得越深，往往能更好地得到结果，因为这减轻了博弈树对于最优性的妥协。
估值函数形式为$Eval(s)=w_1f_1(s)+w_2f_2(s)+\dots$，其中w是权重，f是一个特征（比如己方兵数、对方兵数、己方王数等）。其形式是可以自由定义的，也不一定需要是线性函数，其核心准则是为更好的局面给出更高的分数。当然，要给出一个理想的估值函数需要不断地微调和实验。

### 3.2.2 期望极大算法（Expectimax）
有些时候对手不一定会按照最优方式行动，因而如果总是按照Minimax行动地话，可能会表现得过于悲观。在博弈树中我们引入机会节点（chance nodes），它们不再像最小值节点那样选择子节点的最小值，而是选择子节点的期望值。
即对于机会节点，$V(s)=\sum\limits_{s'\in successors(s)}p(s'|s)V(s')$。
需要注意的是，由于期望值需要遍历所有子节点，因此无法进行原先的alpha-beta剪枝。

典型的代码如下，其余部分与Minimax相同：
```python
def exp_value(self , gameState : GameState , agentIndex , depth):
    total = 0
    legalActions = gameState.getLegalActions(agentIndex)
    for action in legalActions:
        value = self.getValue(gameState.generateSuccessor(agentIndex , action) , (agentIndex + 1) % gameState.getNumAgents() , depth)
        if value is not None:
            total += value
    return total / len(legalActions)
```

### 3.2.3 混合层类型（Mixed Layer Types）
有些时候，并不只有一个对手，因而需要我们灵活地设置最大层，最小层，和机会层。比如说有四个幽灵的吃豆人游戏，就可以用一个最大层后四个最小层来模拟，这将会实现多个幽灵之间的合作。有些时候，我们也可以安排两个幽灵最优行动，两个幽灵随机行动，这就需要添加两个机会层和两个最小层来实现。

## 3.3 广义博弈（General Games）
对于不是零和博弈的游戏，我们对于状态的评分就不是一个数，而是一个元组。各个智能体最大化的是元组中的某一个数值，而最终的选择是综合考虑了各个智能体之间合作的结果，具体如下所示：
![alt text](/assets/images/image-10.png)

## 3.4 蒙特卡洛树搜索（Monte Carlo Tree Search ， MCTS）
实际上，之前的算法只适合求解分支数b较小的情况，对于围棋这种分支数巨大的博弈，无法使用Minimax。这时候，对于某个状态向下的探索，注定只能选取其中很小的一部分。解决该问题的方法是蒙特卡洛树搜索。
如果无法穷尽所有可能行动，对于状态s的评分，我们可以使用某种简单的随机策略进行多次游戏到最终结局，并统计胜率。
当然这里的模拟也有一定说法，比如我们最终选择的是几个状态s中胜率最高的那个，如果对于某个进行了少部分测试得到的胜率已经显著小于其他，那么其实没必要将计算资源浪费在这个状态的模拟上。
此外，胜率并不一定是评分的唯一指标。1/2和500/100的含金量显然不同，因此我们倾向于相信进行了更多次模拟的那个节点。通常，使用如下公式进行节点评分：
$$UCB1(n)=\frac{U(n)}{N(n)}+C\sqrt{\frac{\log N(parent(n))}{N(n)}}$$
N(n)是模拟次数，U(n)是获胜次数，前一项指代胜率，后一项指代模拟次数，C是可以选择的权衡系数，可以自由选择，也可以根据游戏进行的阶段变化。

需要注意的是我们并不是利用这个评分进行最终动作的选择，而是根据这个评分决定多搜索哪种分支情况。MCTS方法由如下几步构成：
（1）从根节点开始，按照UCB标准向下探索，直到找到一个未扩展节点（没有生成所有子状态的节点）；
（2）为该节点添加一个新的子节点，并从这个新的子节点开始运行一次模拟至终局，得到获胜结果；
（3）一路向上回溯，更新路径上的U(n),N(n)以及UCB值。

重复执行以上步骤，最后我们选择根节点的探索次数最多的子节点动作即可（因为会频繁探索更具前途的子节点）。

# 4 马尔科夫决策过程（Markov Decision Processes , MDPs）
## 4.1 MDPs
MDPs处理的问题与传统搜索问题有以下两点不同：
（1）之前我们处理的问题是确定的，即做出什么行动，一定能知道状态如何转移。而在MDPs中，转移模型是不确定的，在状态s下做出行动a，可能会以不同概率得到不同的新状态。
即转移模型$T(s,a,s')$是一个概率，表示在s状态下做行动a得到s'状态的概率；
（2）搜索问题中我们为状态评分，并希望到达得分最高的状态。但有些时候，我们不关心最终到达什么状态，而是关心在整个过程中积累了多少奖励。这时候，我们不再为状态评分，而是为每一个行动给予奖励，统计总行动价值。奖励函数为$R(s,a,s')$，表示在状态s下做行动a并得到新状态s'所获得的奖励。

具体的例子如下，赛车每一步加速还是减速后的状态是不确定的，可能温度变化，也可能温度不变。我们最终的目标是总里程最远，即每一步要尽可能跑的更远。下图中绿色数字表示奖励函数，红蓝表示转移模型：
![alt text](/assets/images/image-11.png)
灰色表示过热，赛车报废，整个过程终止。

为了构建出如之前一样的树结构，我们需要引入虚节点，称为Q状态。它的含义是，从父状态出发，已经选择了某种行动，但为确定得到的新状态的中间过程。其子节点为通过该行动得到的一种新状态，如下所示：
![alt text](/assets/images/image-12.png)

### 4.1.1 终止条件
以上赛车问题是不完善的，显然我们可以一直减速，从而用不过热，并且能得到无限奖励。想要避免该问题，有两种方式：
（1）有限范围（finite horizons），即限定总步数，希望最大化的是这个范围内的奖励最大总和；
（2）折扣因子（discount factors）。即第n步的奖励会带来损失系数$\gamma ^ n,|\gamma | < 1$。因此即使对于无限步数，同样有总奖励$U\leq \sum \gamma^n R_{max} =\frac{R_{max}}{1-\gamma}$为有限值。

### 4.1.2 马尔可夫性（Markovianess）
马尔可夫性性也可以说是无记忆性，即未来和过去条件独立。更直白地，就是转移函数和时间是无关的，不论什么时候，只要在s状态做动作a，总是以同样的概率到达新的s'状态。这种无记忆性对于我们最终所给出的策略同样适用，即最终给出的策略$\pi(s)$只依赖于状态s。
上述例子给人的感觉是，MDP的状态数很小。实际上不然，下棋问题也可以理解成MDP，它的状态数就非常大。其随机性来源于对手的策略。并且不论游戏进行到什么时候，只要盘面是相同的，给出的策略，相应的转移模型都应该是一样的，这体现了无记忆性。

### 4.1.3 贝尔曼方程（Bellman equation）
我们可以用方程的语言清晰地描述所需要满足的最优条件。
首先定义状态s的最优价值，表示从s出发，在其生命周期内可以获得的最大价值，记为$V^*(s)$；
同样定义Q状态的最优价值，记为$Q^*(s,a)$；
有如下方程：
$$V^*(s)=max_aQ^*(s,a)$$
$$Q^*(s,a)=\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V^*(s')]$$

## 4.2 MDPs的求解
### 4.2.1 价值迭代（Value Iteration）
有了贝尔曼方程，我们无非就是要求解满足条件的$V^*(s)$。（这是一个关于s的函数）
联想到迭代法求解方程，我们可以使用迭代法进行求解贝尔曼方程：
$$V_{k+1}(s)=max_a\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V_k(s')]$$
初始条件为$\forall s \in S , V_0(s)=0$。
可以证明，迭代的解将收敛到$V^*(s)$，证明在本节末尾。
有了$V^* (s)$之后，想要给出当前状态的行动非常容易，只需计算得到$Q^* (s,a)$，并选出其中最大的那个所对应的a即可。
事实上，由于给出行动总归要计算$Q^* (s,a)$，我们不如直接迭代求解它，从而整个过程无需$V^*(s)$变量的计算。对应的迭代方程为：
$$Q_{k+1}(s,a)=\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma max_a Q_k^*(s')]$$

**迭代收敛证明：**
证明迭代收敛，只需证明两个估计函数的差在各自进行一轮迭代之后变小即可。
证明过程需要如下关系：
$$|max_zf(z)-max_zh(z)|\leq max_z|f(z)-h(z)|$$
这个关系很好理解，取使得f(z)取最大值的$z_0，LHS\leq f(z_0)-h(z_0) \leq RHS$，假设f(z)更大一些。
利用该关系，可以得到：
$|(max_a\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V(s')])-(max_a\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V'(s')])|$
$\leq max_a|\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V(s')]-\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V'(s')]|$
$=\gamma max_a|\sum\limits_{s'}T(s,a,s')V(s')-\sum\limits_{s'}T(s,a,s')V'(s')|$
$\leq \gamma max_a|\sum\limits_{s'}T(s,a,s') max_{s'}(V(s')-V'(s'))|$
$=\gamma max_{s'}|V(s')-V'(s')|$
$\leq |V(s')-V'(s')|$

典型的价值迭代的代码如下所示：
```python
class ValueIterationAgent(ValueEstimationAgent):
    def __init__(self, mdp: mdp.MarkovDecisionProcess, discount = 0.9, iterations = 100):
        self.mdp = mdp
        self.discount = discount
        self.iterations = iterations
        self.values = util.Counter() # A Counter is a dict with default 0
        self.runValueIteration()

    def runValueIteration(self):
        # 完成的是贝尔曼方程中V(s) = max_a Q(s,a)的一步
        for i in range(self.iterations):
            states = self.mdp.getStates()
            counter = util.Counter()

            for state in states:
                maxx = -1e9
                if len(self.mdp.getPossibleActions(state)) == 0:
                    maxx = 0
                else:
                    for action in self.mdp.getPossibleActions(state):
                        Q = self.computeQValueFromValues(state , action)
                        if Q > maxx:
                            maxx = Q
                counter[state] = maxx
            self.values = counter


    def getValue(self, state):
        return self.values[state]

    def computeQValueFromValues(self, state, action):
        # 完成的是贝尔曼方程中 Q(s,a) = Σ_{s'} T(s,a,s')[R(s,a,s') + γ V(s')]的一步
        # 其中的V(s')用的是当前的估值函数，即self.values
        total = 0
        for next , prob in self.mdp.getTransitionStatesAndProbs(state , action):
            total += prob * (self.mdp.getReward(state , action , next) + self.discount * self.getValue(next))
        return total

    def computeActionFromValues(self, state):
        # 根据最大Q值选出最优策略
        maxx = -1e9
        best_action = None
        for action in self.mdp.getPossibleActions(state):
            Q = self.computeQValueFromValues(state , action)
            if Q > maxx:
                maxx = Q
                best_action = action
        return best_action

    def getPolicy(self, state):
        return self.computeActionFromValues(state)

    def getAction(self, state):
        return self.computeActionFromValues(state)

    def getQValue(self, state, action):
        return self.computeQValueFromValues(state, action)
```

### 4.2.2 策略迭代（Policy Iteration）
价值迭代中，每一次迭代更新，需要计算|S|个$V_{k+1}(s)$，其中每一个迭代式需要枚举所有状态和动作，因此一次迭代的时间复杂度为$O(|S|^2|A|)$。如此进行迭代的收敛速度较慢，本节介绍另一种迭代方式——策略迭代。
对于MDP而言，核心是求解策略函数$\pi(s)$。（注意由于马尔可夫性，策略也仅和状态相关，与时间无关）我们希望迭代地求解$\pi(s)$。
对于已知的$\pi^i(s)$，可以迅速地求出:
$V^\pi(s)=\sum\limits_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+\gamma V^\pi(s')]$
这是线性方程组，无需迭代求解。
然后根据已知的$V^\pi(s)$，可以按照如下迭代式更新策略：
$\pi_{i+1}(s)=argmax_a\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma V^{\pi_i}(s')]$
如果得到$\pi_{i+1}=\pi_i$，说明已经收敛，即得到了最终策略$\pi^*(s)$。

# 5 强化学习（Reinforcement Learning ， RL）
## 5.1 强化学习定义
上一章中解决的MDP问题，针对于已知转移模型和奖励函数的情况。但通常而言，这两者是未知的，我们只能通过进行操作获得反馈（包括新状态和奖励）。这就需要我们设计一种通过尝试并利用反馈估计最优策略的智能体。这种通过尝试学得策略而非直接告知有关模型的过程称之为强化学习。
强化学习的过程中，我们每次行动尝试会获得一个反馈(s,a,s',r)后两者是新状态和奖励，称之为一个样本（sample）。一轮尝试到达终止后所有样本的集合称为一个回合（episode）。智能体需要先经历许多回合，学到足够数据后方可给出策略。

强化学习分为两种类型，基于模型的学习和无模型学习。前者是通过尝试猜出转移函数和奖励函数，再按照上一章中的迭代方法求解MDP。后者则是直接估计价值状态，无需得到估计的转移模型和奖励函数。

## 5.2 基于模型的学习（Model-Based Learning）
通过尝试，我们得到了大量的(s,a,s',r)。每一个样本对应了一个已知或未知的奖励函数R(s,a,s')=r。存储下来即可得到对于奖励函数的估计。
同时我们可以统计各个(s,a)对应的s'的次数，并归一化得到转移模型，即给定s,a得到s'的概率T(s,a,s')。

基于模型的学习很简单，但是维护(s,a,s')进行计数和存储的内存开销可能很大。为解决该问题，需要使用无模型的学习。

## 5.3 无模型学习（Model-Free Learning）
### 5.3.1 被动强化学习
被动强化学习其实严格来说并不是无模型学习。它的本质就是策略迭代算法。策略迭代算法分为两步，第一步是通过给定策略得到各个状态的估值函数；第二步是利用估值函数迭代得到新策略。在迭代算法中，前后两步都需要知道转移模型和奖励函数，而被动强化学习指的就是可以在不知道模型的情况下完成第一步。有两种典型的被动强化学习方法：

#### 5.3.1.1 直接评估
直接评估是用公式给出状态估值。具体地，根据给定的策略进行探索，我们可以得到从每个状态出发到终止节点的总收益（注意要获取每个中间状态）以及访问到每个状态的总次数，二者相除就是在当前策略下的状态估值。
具体展示如下：
![alt text](/assets/images/image-13.png)
![alt text](/assets/images/image-14.png)
然而这种方法得到的状态估值往往具有较大的偶然性，需要足够多次数的尝试才能得到好的状态估值，然而这仅仅只是对于一个策略迭代的中间步骤的开销。因此该方法并不理想。

#### 5.3.1.2 时间差分学习（Temporal Difference Learning）
有别于直接给出公式，时间差分学习通过类似于迭代的方式得到对于给定策略的状态估值：
$sample_k = R(s,\pi(s),s') + \gamma V^\pi_k(s')$，
$V^\pi_k(s)=(1-\alpha)V^\pi_{k-1}(s)+\alpha \cdot sample_k,\alpha$为学习率，
可以看到随着迭代次数k的增加，越早的sample值的贡献越少$((1-\alpha)^{k-i}sample_i)$，而它们正好也是最不准确的。通过这种算法，使用较少的回合数就可以收敛到真实的状态估值函数。
这里没有严格说明为什么收敛到真实的状态估值，但是从感觉上可以理解。首先，与贝尔曼方程相比，我们缺少的是权重信息，这里通过多次采样完成了对该权重的模拟；然而在多次采样的过程中，必须使用已有的不准确的估值，为了确保这里的噪声最终不会影响，我们需要使得最开始最不准确的噪声在之后的贡献是充分小的，而学习率的指数衰减保证了这一点。

### 5.3.2 主动强化学习
#### 5.3.2.1 Q学习
之前的想法总是希望学习出一个合理的估值函数V(s)，然而为了得到最终的策略，我们还是需要找Q(s)。因此我们尝试直接求解Q值。Q值迭代的贝尔曼方程如下所示：
$$Q_{k+1}(s,a)=\sum\limits_{s'}T(s,a,s')[R(s,a,s')+\gamma max_{a'} Q_k(s',a')]$$
这个形式，实际上和上面的TD学习要求解的形式几乎相同，仅仅多了一个对于有噪声Q值的取最大值操作。因此直接使用类似的采样并迭代的方式可以直接学得Q(s,a)，并可以立即给出各状态对应的策略:
$sample_k = R(s,a,s') + \gamma max_{a'} Q_k(s',a')$，
$Q_k(s,a)=(1-\alpha)Q_{k-1}(s,a)+\alpha \cdot sample_k,\alpha$为学习率，
典型的Q学习代码如下：
```python
class QLearningAgent(ReinforcementAgent):
    def __init__(self, **args):
        ReinforcementAgent.__init__(self, **args)
        self.QValues = util.Counter()

    def getQValue(self, state, action):
        # 没有计算过的Q态，返回0
        if (state , action) not in self.QValues.keys():
            return 0
        
        return self.QValues[(state , action)]

    def computeValueFromQValues(self, state):
        # 完成Q学习更新方程中求最大值的那一步
        actions = self.getLegalActions(state)
        maxx = -1e9
        # 注意对终止状态，无合法行动，要返回Q值为0
        if len(actions) == 0:
            return 0
        for action in actions:
            QValue = self.getQValue(state, action)
            if QValue > maxx:
                maxx = QValue
        return maxx

    def computeActionFromQValues(self, state):
        actions = self.getLegalActions(state)
        maxx = -1e9
        best_action = None
        for action in actions:
            QValue =  self.getQValue(state , action)
            if QValue > maxx:
                maxx = QValue
                best_action = action
        return best_action

    def update(self, state, action, nextState, reward: float):
        # 完成Q学习中的取样更新步骤
        if nextState:
            QValue = (1 - self.alpha) * self.getQValue(state , action) + self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState))
        else:
            QValue = (1 - self.alpha) * self.getQValue(state , action) + self.alpha * reward
        self.QValues[(state , action)] = QValue
    
    def getPolicy(self, state):
        return self.computeActionFromQValues(state)

    def getValue(self, state):
        return self.computeValueFromQValues(state)
```

#### 5.3.2.2 近似Q学习
Q学习直接通过强化学习的方式给出了最优策略，无需对于模型的估计。然而，要想存储下所有的Q(s,a)，需要的内存是相当大的。可以使用如下方法优化存储：
为了不存储每一个状态对应的估值，我们需要将状态用一系列特征$f_i$描述出来。以吃豆人为例，我们可以用最近食物距离，最近鬼魂距离，鬼魂数量，是否被围困等特征来表达。
根据我们所描述的特征，可以得到状态在该特征下的投影$f_i(s),f_i(s,a)$。这两个函数无需存储，可以通过我们给出的特征描述针对不同(s,a)直接计算得到。
于是状态估值可以表述为如下形式：
$V(s)=w_1f_1(s)+w_2f_2(s)+\dots w_nf_n(s),$
$Q(s,a)=w_1f_1(s,a)+w_2f_2(s,a)+\dots w_nf_n(s,a),$
我们只需要存储权重$(w_1,w_2\dots w_n)$即可得到估值函数，所需的内存大小取决于我们找出的特征数目。
使用如下方式迭代求解权重，其本质是梯度下降。
记$\Delta = [R(s,a,s')+\gamma max_{a'}Q(s',a')]-Q(s,a)$，我们的目的就是计算出更合理的Q(s,a)，使得$\Delta ^ 2$更小。
因此只需要按照$w_i \leftarrow w_i - C \cdot \frac{\partial \Delta^2}{\partial w_i}$的更新规则进行迭代即可，
其中C为可以自选的系数，于是后一项写为：$ - C' \cdot \Delta \cdot \frac{\partial Q(s,a)}{\partial w_i}=-C'\cdot \Delta \cdot f_i(s,a)$。
合并起来，得到如下更新规则：
$$w_i\leftarrow w_i + \alpha \cdot \Delta \cdot f_i(s,a),\alpha为学习率$$
典型的代码如下所示：
```python
def update(self, state, action, nextState, reward: float):
    delta = (reward + self.discount * self.getValue(nextState)) - self.getQValue(state , action)
    features = self.featExtractor.getFeatures(state , action)
    for item in features:
        self.weights[item] += self.alpha * delta * features[item]
```

#### 5.3.2.3 探索函数（Exploration Functions）
在Q学习的过程中我们总是要反复进行采样，而每次采样会用到已经得到的含噪声的Q值中的最大值：
$sample_k = R(s,a,s') + \gamma max_{a'} Q_k(s',a')$，
此时，如果每次采样都选择一样的最大值，那么意味着并没有对所有情况进行充分探索；而要使得最终结果收敛到真实的最大值，又必须使得最终应该总是访问到相同的最大值上。因此可以进行如下改造：
$sample_k = R(s,a,s') + \gamma max_{a'} f_k(s',a')$，
$f_k(s,a)=Q(s,a)+\frac{A}{N(s,a)},N(s,a)$表示Q状态(s,a)被访问的次数，A是自定义参数；
利用这种方法，在探索初期会给没被探索过的状态较高的权重，而在探索结束时，由于总是探索相同状态，附加的权重会逐渐收敛，实现了自动的探索权重分配。

# 6 贝叶斯网络（Bayes Nets）
## 6.1 概率论基础
给出如下概念：
联合分布，即概率和多个变量有关,$P(A,B,C)=P(A,C,B)$；
条件概率，在B发生的条件下发生A的概率记为$P(A|B)$；
|之后代表条件，即已经给定的部分；
边缘分布，就是把不关注的变量求和掉，$P(A,B)=\sum\limits_cP(A,B,C=c)$；
显然可以看出$P(A,B)=P(A|B)P(B)$，这就是所谓的贝叶斯规则；
两个随机变量相互独立，记作$A\perp\!\!\perp B$，显然有如下等式：
$P(A,B)=P(A)P(B),P(A|B)=P(A),P(B|A)=P(B)$;
条件独立，即A和B在随机变量C给定的情况下独立，记作$A\perp\!\!\perp B|C$,显然有如下等式：
$P(A,B|C)=P(A|C)P(B|C),P(A|B,C)=P(A|C),P(B|A,C)=P(B|C)$；
可以看出实际上就是在独立的等式各个部分都增加了条件C。

## 6.2 贝叶斯网络
假设有n个变量，每个变量有d种情况，总共有$d^n$种情况，每个情况都有各自的概率。想要知道任意概率分布，最直接的方法，是存储下巨大的$d^n$个概率组成的表，然后在表格中进行查询。这显然是不现实的，因为存储量过大。而贝叶斯网络通过存储条件概率的方式实现对于任何概率分布的表达。

贝叶斯网络是一个有向无环图（DAG），每个节点对应一个变量X，在每个节点上存储条件分布$P(X|A_1\dots A_n)$，其中$A_i$是X的父节点。根据在每个节点上的条件分布，就可以出任意的概率分布。显然存储多个小的概率分布表的开销小于存储一个大的概率分布表。每个节点上的条件概率表（Conditional Probability Table）简称为CPT。

贝叶斯网络的具体例子如下：
![alt text](/assets/images/image-15.png)
每一个单项箭头代表了二者之间存在因果联系。
存储的值为$P(B),P(E),P(A|B,E),P(J|A),P(M|A)$；
任意概率分布可以通过存储的概率求得：
$P(-b,-e,a,j,-m)=P(-b)P(-e)P(a|-b,-e)P(j|a)P(-m|a)$。

需要注意的是上面式子的成立并不是显然的，其中蕴含了条件独立性：
直接按照链式法则展开，有$P(B,E,A,J,M)=P(B)P(E|B)P(A|B,E)P(J|A,B,E)P(M|A,J,B,E)$,
于是上式成立需要$P(J|A,B,E)=P(J|A),P(M|A,J,B,E)=P(M|A)$。
这两个式子体现出贝叶斯网络的两种独立性规则：
（1）父节点给定，则节点条件独立于其所有祖先节点。
由此第一式自然成立，而第二式还需证明$P(M|A,J)=P(M|A)$；
这个结论是容易理解的，父节点给定之后，当前节点的概率已经被锁定了，与祖先的状态无关；
（2）一个节点的父节点，子节点，以及子节点的其他父节点给定，则该节点与其余所有节点都条件独立。这里给定的部分把该节点包的严严实实，没有任何泄漏信息的可能，称之为马尔科夫地毯。
应用此式可以证明等式的剩余部分。这里需要理解的是，子节点给定，共父之间是会传递信息的。A和B都有可能导致C，如果C发生了，A不发生的话则B必然发生，依次A和B是不独立的。
这里我们只给出了理解，严格证明见下一节。

## 6.3 D分离
以上的独立性规则，是我们能够画出贝叶斯网络的原因。但是通产来说，我们很难清晰地判断出这些独立性与条件独立性关系（统计规律），却能直觉（通过因果规律）地给出贝叶斯网络。因此通常也会用贝叶斯网络画出来的结构反过来解释其中蕴含的条件独立规律。本节更深入地体会这种关系：

### 6.3.1 因果链（Causal Chains）
考虑$X\to Y\to Z$的因果链，有关系：$P(X,Y,Z)=P(Z|Y)P(Y|X)P(X)$。
贝叶斯独立规律告诉我们X和Z在Y给定情况下条件独立。（注意二者本身不独立，假设$P(y|x)=\delta_{x,y},P(z|y)=\delta_{z,y}$，设x给定，则y=x,z=y=x，因此不独立）
我们试图利用上面的式子证明条件独立性：$P(X|Z,Y)=P(X|Y)$。
证明过程如下：
$P(X|Z,y)=\frac{P(X,Z,y)}{P(Z,y)}=\frac{P(Z|y)P(y|X)P(X)}{\sum\limits_xP(x,y,Z)}=\frac{P(Z|y)P(y|X)P(X)}{P(Z|y)\sum\limits_xP(y|x)P(x)}=\frac{P(y|X)P(X)}{\sum\limits_xP(y|x)P(x)}=\frac{P(y|X)P(X)}{P(y)}=P(X|y)$；
注意其中第二三个等号用到了一开始给出的等式，最后一个等号用到了贝叶斯规则。

### 6.3.2 共同原因（Common Cause）
$X \leftarrow Y \to Z$。即Y是X，Z的共同原因。根据贝叶斯网络，有关系：$P(X,Y,Z)=P(X|Y)P(Z|Y)P(Y)$。
注意X和Z本身并不独立。仍然考虑上述$\delta$函数的例子，x给定则z也被确定下来了。
但是Y给定时，X，Z条件独立，即证明$P(X|Z,Y)=P(X|Y)$。
证明过程如下：
$P(X|Z,y)=\frac{P(X,Z,y)}{P(Z,y)}=\frac{P(Z|y)P(X|y)P(y)}{P(Z|y)P(y)}=P(X|y)$

### 6.3.3 共同影响（Common Effect）
$X \to Y \leftarrow Z$。即X，Z共同影响Y。注意这时候不能举上面的反例，因为我们只有对Y的概率分布的一个约束，即只能在同时给定了X，Z两个变量之后才能确定Y的概率分布。
根据贝叶斯网络，有关系：$P(X,Y,Z)=P(Y|X,Z)P(X)P(Z)$。
首先，这时给定Y时，X和Z之间并不独立。
假设X，Z各有百分之五十概率为0，1。P(y|x,z)= (y == (x xor z))。即y正确地给出了x，z是否相同的判断则为1，错误地判断了x，z是否相同则为0。此时如果知道y，就能够知道x，z是否相同了，二者并不独立。
我们可以用贝叶斯网络的关系得出X,Z之间相互独立，即$P(X,Z)=P(X)P(Z)$，证明是很显然的：
$P(x,y,z)=P(y|x,z)P(x,z)=P(y|x,z)P(x)P(z)$
需要说明的是，如果Y的某个后代W被观测，在条件W下，X和Z也不条件独立。我们可以使得Y到后代上的概率分布保证其后代必须和Y都相同，再结合之前反例，仍可知道破坏了条件独立性。

### 6.3.4 D分离
我们现在希望解决的问题是，给定贝叶斯网络结构之后，判断X，Y变量是否在${Z_1,Z_2\dots Z_k}$被观测后条件独立。
根据以上三种情况，只要从X到Y，能够设计一种概率分布的传播方式破坏独立性的话，那么二者在当前条件下必然是不独立的。而能在当前条件下（某些节点被观测）传播独立性破坏的连接方式称为d连接。连接导通需要使得各组分满足如下三种形式：
![alt text](/assets/images/image-16.png)
对于这三种形式（左右节点可以被染色，不影响导通），只要按照上面的反例设计概率分布的传播，必定导致首尾（左右）节点之间不满足条件独立。

更程序化的判断方式如下：
（1）为所有观测点着色；
（2）不计边的方向性，只考虑连通，判断所有可达路径是否是d连接的；
（3）对于一条路径（这时需要考虑方向），如果每3个节点构型都属于上图中的一种，说明这条路径是d连接的；
（4）如果所有路径都不是d连接的，那么X和Y之间在已观测条件下非条件独立。
换句话说，只要找到一条d连接路径，说明不条件独立。

## 6.4 贝叶斯网络中的推断
前面介绍了贝叶斯网络，它通过因果关系结构可以实现对全局概率分布的高效存储，即理论上整个巨大的概率分布表格都被存储在这个网络结构中。但是我们的目的是得到任意的$P(Q_1Q_2\dots|e_1e_2\dots)$，还需要一个方法从贝叶斯网络中算出这个概率分布。

### 6.4.1 精确推断：变量消除
显然我们都能算出总的联合概率分布函数，也能够精确计算得到任何给定的概率分布。

每个节点X存储了一个函数$P(X|A_1A_2\dots)$，它的自变量是$X,A_1,A_2\dots$。将所有顶点存储的函数相乘得到的就是联合概率分布$P(Q_1Q_2\dots Q_N)$。
想要求得结果无非是确定其中某些变量的值，即把自变量变成给定参量，放在条件栏右侧；以及求和消掉某些变量。
想要求和消掉某些变量，即是对某些节点进行求和，而这些求和并不需要在执行完联合分布函数的求解之后再进行，而是可以先进行这些求和，再完成剩余的乘法，这样能够有效减少存储。这种在中间过程进行求和的方法叫做变量消除。
在中间过程求和，只需要对以当前求和变量为自变量的所有待求乘法的函数进行求积与求和操作，并打包生成新的函数，纳入待求乘法的函数序列中。

完成所有的求和后，再完成剩余的乘法即可获得待求的概率分布函数。需要注意的是这有可能是未归一化的，最后需要进行归一才能得到正确的概率分布。

变量消除的典型代码如下所示：
```python
factors = bayesNet.getAllCPTsWithEvidence(evidenceDict)
for eliminate_var in eliminationOrder:

    factors , joined_factor = joinFactorsByVariable(factors , eliminate_var)
    if len(joined_factor.unconditionedVariables()) > 1:
        eliminate_factor = eliminate(joined_factor , eliminate_var) # 这里相当于完成求和操作
        factors.append(eliminate_factor)
    # 稍微优化了一下，常数因子不重要，因为反正都要归一化

final_factor = joinFactors(factors)
final_factor = normalize(final_factor)
return final_factor

def eliminate(factor: Factor, eliminationVariable: str):
    unconditioned_vars = factor.unconditionedVariables()
    unconditioned_vars.remove(eliminationVariable)
    conditioned_vars = factor.conditionedVariables()

    new_factor = Factor(unconditioned_vars , conditioned_vars , factor.variableDomainsDict())

    for assignment in new_factor.getAllPossibleAssignmentDicts(): # 相当于获取概率函数的所有自变量范围
        prob = 0
        for val in factor.variableDomainsDict()[eliminationVariable]:
            # 这里要把带消除变量按照设定值加回来当作原先因子的自变量
            assignment_copy = assignment.copy()
            assignment_copy[eliminationVariable] = val 
            prob += factor.getProbability(assignment_copy)
            
        new_factor.setProbability(assignment , prob)
        
    return new_factor
```

### 6.4.2 近似推断：采样
虽然说计算出概率分布函数是困难的，但是直接根据贝叶斯网络进行采样却是简单的。我们可以直接根据采样的结果推算出需要的概率，这样做大幅减少了计算量。

#### 6.4.2.1 先验采样（Prior Sampling）
![alt text](/assets/images/image-17.png)
对于如上贝叶斯网络，可以很容易通过以下采样模拟：
```python
import random

def get_t():
    if random.random() < 0.99:
        return True
    return False

def get_c(t):
    if t and random.random() < 0.95:
        return True
    return False

def get_sample():
    t = get_t()
    c = get_c(t)
    return [t, c]

```
不过如果想要计算P(C|-t)，在对t进行模拟时会舍弃掉百分之九十九的样本，较为浪费。

#### 6.4.2.1 拒绝采样（Rejection Sampling）
对以上采样可行的优化是，对于不再最终关心范围内的样本，尽早拒绝。比如我们关心的是最终为-t的情况，于是如果生成的T为负值，则无需再对C进行采样赋值。这种做法仍然需要丢弃样本，但至少节约掉了生成坏样本的时间。

#### 6.4.2.2 似然加权（Likelihood Weighting）
也可以直接不对待求而对$P(Z_1Z_2\dots |e_1e_2\dots)$中的证据变量e进行采样，而是之间声明其为观测值。
这样的话，得到的采样权重并不是真实的联合概率分布函数，缺少了证据变量节点的那部分CPT。为了补偿这部分概率，我们将其纳入各个采样值的权重之中。
换句话说，现在各个采样值具有不同的权重，默认为1。在进行一次采样时，对于非证据变量，我们正常采样；但对于证据变量，我们将其CPT乘到权重上。这样得到的有权重的采样结果的概率分布与总的概率分布相同。

在以上三种方法中，似然加权的计算效率最高。

#### 6.4.2.3 吉布斯采样（Gibbs Sampling）
对于一个充分大的网络，按照整体的逻辑顺序完成采样通常是很复杂的。吉布斯采样的方法每次只需依照周围节点的CPT即可完成采样，无需实现整个贝叶斯网络的逻辑。
具体的，我们通过如下迭代方式生成一个样本：
初始时随机选取非证据变量，按最终结果赋值证据变量。此时所有变量都已被赋值；
按顺序抽取一个非证据变量X进行采样，采样依照的分布为$P(X|A_1,A_2\dots)$，其中$A_1,A_2\dots$为X的所有邻居，包括父节点和子节点，用采样值更新该非证据变量的值，此时的整个样本为吉布斯采样的一个样本。
可以证明按照这种采样方法最终也能够收敛到正确的概率分布函数上，并且每次采样只需要进行一个仅考虑周围节点的采样。

需要说明一下如何根据CPT得到$P(X|A_1,A_2\dots)$。事实上，由于其他变量均已确定，并且由于马尔科夫地毯所决定的条件独立性，有:
$P(X|A_1,A_2\dots A_n)\propto P(X,A_1,A_2\dots A_n , B_1 , B_2 \dots) = P(X,A_1,A_2\dots A_n )\propto CPT(X)CPT(A_1)CPT(A_2)\dots CPT(A_n)$，
即CPT乘积所得的函数是正比于我们想要的概率分布函数的，只需重新归一化即可。
这里正比是因为其余变量均已确定，同时CPT乘积多出的那部分外围的贡献总是条件无关的，因此无需考虑。

# 7 决策网络
## 7.1 效用函数
我们要让我们所编写的智能体帮我们做决策，通常需要告诉智能体在做选择时的偏好。如果是偏好设计的好，智能体只需要简单地使分数（效用）最大化就行了。但是有些时候我们的偏好设计可能是非理性的。
比如说A，B，C三件商品。你告诉智能体：
（1）愿意用A加上1块钱换购B；
（2）愿意用B加上1块钱换购C；
（3）愿意用C加上1块钱换购A；
显然这样的偏好设计使得A，B，C的价值排序是有矛盾的，这会导致智能体不断地换购然后一直花钱。

对于理性的偏好设计，我们只需要智能体始终追求分数最大化即可。需要注意的是，这里的分数并不一定是真实的奖励，还包含了对于风险的主观接受程度：
比如说有两种选择:A是(0.5:0,0.5:1000),B(1:500)。":"前表示概率，":"后表示真实奖励x。
但我们告诉智能体的用来评价的分数不一定要是真实奖励，而可以是它的函数U(x)。
如果$U(x)=x$，那么选择A和B的期望得分都是500，智能体会认为选择哪一个无所谓；
如果$U(x)=\sqrt(x)$，A的期望是15.81，B的期望是22.36，智能体会选择B，表现出规避风险；
如果$U(x)=x^2$，A的期望是500000，B的期望是250000，智能体会选择A，表现出追求风险；
可以看到，效用函数U(x)的选择，不仅包含了奖励多少的信息，也包含了我们希望智能体对于风险采取怎样的态度的主观要求。

## 7.2 决策网络（Decision Networks）
希望智能体进行的决策问题可以用决策网络表述。决策网络综合了之前的贝叶斯网络与期望极大等算法。具体的例子如下：
![alt text](/assets/images/image-18.png)
我们想让智能体帮我们推断，已知天气预报会下雨的情况下，我们是否需要带伞。
注意这个决策网络中有三种不同类型的节点：
矩形节点：动作节点，表示做某种动作，是确定性的。
椭圆节点：机会节点，可以认为是一个信息或者变量，其对子节点的影响不是确定的，而是有概率的。
菱形节点：效用节点，根据其父节点（包括机会节点和动作节点）打出一个分数。

效用函数是给定的，即根据动作和某些变量的结果给出确定分数。
效用函数中所需要的概率分布可以通过贝叶斯网络得出。我们知道下雨的概率P(W)和天气预报的准确率P(F|W)，也可以通过贝叶斯网络反推出我们需要的后验概率P(W|F=bad)。
利用概率分布，我们可以知道在已知证据变量的情况下做出不同行为得到的期望分数：
EU(leave|bad)=0.34·100+0.66·0=34，
EU(take|bad)=0.34·20+0.66·70=53，
因此智能体会告诉我们，在已知天气预报会下雨的情况下，应该带伞。

用结果树（Outcome Trees）可以更清晰地表达以上决策过程：
![alt text](/assets/images/image-19.png)
这与期望最大化（Expectimax）算法的不同仅在于我们知道了一些证据变量，其中的后验规律是通过贝叶斯网络反推得到的。

## 7.3 完全信息价值（Value of Perfect Information，VPI）
上一节中，我们能够得到知道天气预报之后应该做出什么行动。但是查看天气预报这件事本身是有代价的。我们需要知道查看天气预报是否有助于我们进行判断，即需要将获得某个证据变量的成本与价值进行比较，从而决定是否值得进行观察。

### 7.3.1 通用公式
观测证据e的效用是：
$$MEU(e)=max_a\sum\limits_sP(s|e)U(s,a)$$
再观测一个新证据e'的效用是：
$$MEU(e,e')=max_a\sum\limits_sP(s|e,e')U(s,a)$$
但是我们只能选择是否进行一个新的观测，并不能知道观测的变量具体是什么，以及观测出来的情况具体是什么，因此只能当作是一个随机变量E'：
$$MEU(e,E')=\sum\limits_{e'}P(e'|e)MEU(e,e')$$
因此，进行一个新的观测的价值是：
$$VPI(E'|e)=MEU(e,E')-MEU(e)$$
依旧以之前的例子做解释：
![alt text](/assets/images/image-20.png)
不进行任何观测的效用函数：
$MEU(\emptyset)=max_a\sum\limits_wP(w)U(a,w)=max\{0.7\cdot 100 + 0.3\cdot 0,0.7\cdot 20+0.3\cdot 70\}=max\{70,35\}=70$
已经知道$MEU(F=bad)=53,MEU(F=good)=95$，
$MEU(e,E')=P(F=good)MEU(F=good)+P(F=bad)MEU(F=bad)=0.59\cdot95+0.41\cdot53=77.78$
于是进行一次新观测的价值是：
$VPI(F)=MEU(F)-MEU(\emptyset)=77.78-70=7.78$

### 7.3.2 VPI性质
**非负性**：观测新信息总是可以进行更明智的决策。
**非可加性**：$VPI(E_i,E_j|e)\neq VPI(E_i|e)+VPI(E_j|e)$，
观测两个证据的期望效用并不是分别观测各个证据的和。因为观测一个变量，可能会影响观测另一变量的价值。因此，观察两个新的证据变量的VPI应该等于先观察其中一个，并纳入当前证据中，然后再观察另一个。
**顺序独立性**：即上述观测两个新的证据变量的顺序并不重要。因为我们尚未采取任何行动，仅仅只是观察而已。

# 8 马尔科夫模型
## 8.1 马尔科夫模型
之前所学的贝叶斯网络，给人的感觉就好像只能解决很少的几种状态。想要模拟类似于下棋这种，一步一步的行为，即引入时间的维度，状态会变得非常多，导致问题无法处理。本节我们希望研究含有时间步数的贝叶斯网络，从最具有结构特征的马尔科夫模型入手。

马尔科夫模型是一个链状的贝叶斯网络，且转移模型是平稳的（即各个节点的CPT是相同的）。因此只需要两个表来表示马尔科夫模型:$P(W_0),P(W_{i+1}|W_i)$。

### 8.1.1 迷你向前算法（The Mini-Forward Algorithm）
马尔科夫模型存储的是一些条件概率，想要知道每个节点单独的概率分布，可以通过如下方法：
$P(W_{i+1})=\sum\limits_{W_i}P(W_i,W_{i+1})=\sum\limits_{W_i}P(W_{i+1}|W_i)P(W_i)$
即我们可以用递推的方式，依次计算出$P(W_0),P(W_1),P(W_2)\dots$
有了递推之后，我们自然需要知道，由递推求得的概率是否会收敛？

### 8.1.2 稳态分布（Stationary Distribution）
将$P(W_i)$视为向量x，$P(W_{i+1}|W_i)$视为矩阵A，求解收敛问题即为求解线性方程Ax=x。满足该方程的概率分布称为稳态分布。

## 8.2 隐式马尔科夫模型（Hidden Markov Models，HMM）
HMM允许我们在之前的基础上，对每一个非初始节点进行观测，其贝叶斯网络如下（注意箭头的方向）：
![alt text](/assets/images/image-21.png)
其中只有三个需要维护的参数：$P(W_0),P(W_{i+1}|W_i),P(F_i|W_i)$。
用一个具体的例子来解释，W是天气（是否下雨），F是天气预报。我们给出的三个概率是第一天的天气情况；当天的天气情况对前一天天气情况的依赖；当天的天气预报对当天天气情况的依赖（可以理解成天气预报的正确率）。

定义如下两个变量：
$B(W_i)=P(W_i|f\{1:i\})$，即如果每天都看天气预报，那么今天下雨的概率；
$B'(W_i)=P(W_i|f\{1:(i-1)\})$，即如果每天看天气预报，就今天没看的下雨的概率。

### 8.2.1 向前算法
可以给出如下递推关系：
$B'(W_{i+1})=P(W_{i+1}|f_1,\dots ,f_i)=\sum\limits_{w_i}P(W_{i+1},w_i|f_1\dots ,f_i)= \sum\limits_{w_i}P(W_{i+1}|w_i,f_1\dots ,f_i)P(w_i|f_1,\dots ,f_i)$；
注意到在$W_i$的条件下，$W_{i+1}$与所有之前的f是条件独立的（共同原因情形）。而上式的最后一项就是$B(w_i)$。
于是有：
$$B'(W_{i+1})=\sum\limits_{w_i}P(W_{i+1}|w_i)B(w_i)$$
但这并不够，我们想求的是$B(W_i)$与$B(W_{i+1})$之间的关系。可以用链式法则展开：
$B(W_{i+1})=P(W_{i+1}|f_1,\dots,f_{i+1})=\frac{P(W_{i+1},f_{i+1}|f_1,\dots,f_i)}{P(f_{i+1}|f_1,\dots,f_i)}\propto P(W_{i+1},f_{i+1}|f_1,\dots,f_i)$
正比于是因为分母可以当作常数，而分母可以当作常数是因为在求$B(W_{i+1})$这个函数的时候，$f_1\dots f_{i+1}$都是条件，都是已知的。注意这里用了正比于，因此最后要对得到的概率函数重新做归一化。
继续展开可以得到：
$P(W_{i+1},f_{i+1}|f_1,\dots,f_i)=P(W_{i+1}|f_1,\dots,f_i)P(f_{i+1}|W_{i+1},f_1,\dots,f_i)=B'(W_{i+1})P(f_{i+1}|W_{i+1})$
最后一步用了条件独立性，共同原因阻止了d连接。
于是可以得到真正的递推关系式，其仅仅依赖于已知的三个概率分布函数：
$$B(W_{i+1})\propto P(f_{i+1}|W_{i+1})\sum\limits_{w_i}P(W_{i+1}|w_i)B(w_i)$$
注意在实际从前向后递推计算某一个B值时，无需在中间过程进行归一化。只需在最后一步统一进行归一即可，这样大大缩短了求解时间。
计算这一步更新时，通常拆分成两个部分：
时间流逝更新（Time Elapse Update），对应于$P(W_{i+1}|w_i)$;
观察更新（Observation Update），对应于$P(f_{i+1}|W_{i+1})$。
典型的代码如下所示：
```python
def elapseTime(self, gameState: busters.GameState):
    # 时间流逝更新 B(W_{i+1}) = \sum_{w_i} B(w_i) * P(W_{i+1}|w_i)
    # 我们并不是依次计算每个W_{i+1}
    # 而是依次取出不同w_i对应的P(W_{i+1}|w_i)
    # 算出贡献加到对应的B(W_{i+1})上

    beliefs = DiscreteDistribution()

    for old_pos in self.allPositions:
        pos_dict = self.getPositionDistribution(gameState , old_pos)
        # 获取P(W_{i+1}|w_i)

        for new_pos , prob in pos_dict.items():
            beliefs[new_pos] += self.beliefs[old_pos] * prob
            # 进行求和，\sum_{w_i} B(w_i) * P(W_{i+1}|w_i)

    self.beliefs = beliefs
    self.beliefs.normalize()

def observeUpdate(self, observation: int, gameState: busters.GameState):
    pacman_position = gameState.getPacmanPosition()
    jail_position = self.getJailPosition()

    # 观察更新：B(W_{i+1}) = P(f_{i+1}|W_{i+1}) * B(W_{i+1})
    beliefs = DiscreteDistribution()

    for ghost_position in self.allPositions:
        if ghost_position == jail_position and observation is not None:
            beliefs[ghost_position] = 0
        else:
            # 取出P(f_{i+1}|W_{i+1})
            prob = self.getObservationProb(observation , pacman_position ,ghost_position ,  jail_position)
            beliefs[ghost_position] = prob * self.beliefs[ghost_position]

    self.beliefs = beliefs
    self.beliefs.normalize()
```

## 8.3 维特比算法（Viterbi Algorithm）
在HHM中我们关心另一个问题。如果知道每天的天气预报，那这些天的天气情况序列最有可能是什么？
首先直译这个问题，即求解$argmax_{x_{1:N}}P(x_{1:N}|e_{1:N})$，
可以用链式法则化简该式，即求解$argmax_{x_{1:N}}P(x_{1:N},e_{1:N})$。被丢掉的那部分$P(e_{1:N})$是已知的，并不影响我们找寻最大值序列。
而我们有$P(X_{1:N},e_{1:N})=P(X_1)P(e_1|X_1)\prod\limits_t P(X_t|X_{t-1})P(e_t|X_t)$，连乘部分仅仅依赖于各个节点自身，因此问题可以转化为：
![alt text](/assets/images/image-22.png)
已知每条边的权重，希望找到一条路径使得权重的累乘最大化。

可以利用动态规划的方法求解最大路径。$m_t[x_t]$存储的是在t节点选定$x_t$的最大路径总权重，其满足状态转移方程：
$m_t[x_t]=max_{x_{1:t-1}}P(x_{1:t},e_{1:t})=max_{x_{1:t-1}}P(e_t|x_t)P(x_t|x_{t-1})P(x_{1:t-1},e_{1:t-1}=P(e_t|x_t)max_{x_{t-1}}P(x_t|x_{t-1})max_{x_{1:t-2}}P(x_{1:t-1},e_{1:t-1})=P(e_t|x_t)max_{x_{t-1}}P(x_t|x_{t-1})m_{t-1}[x_{t-1}]$
要求最大路径权重，只需求$max_{x_t}m_t[x_t]$即可。而要求解路径，我们还需要维护每个$m_t[x_t]$对应的上一个节点的选择:
$a_t[x_t]=argmax_{x_{t-1}}P(e_t|x_t)P(x_t|x_{t-1})m_{t-1}[x_{t-1}]=argmax_{x_{t-1}}P(x_t|x_{t-1})m_{t-1}[x_{t-1}]$
最终获取路径的方式是：
$x_N=argmax_{x_N}m_N[x_N];x_{N-1}=a_N[X_N];x_{N-2}=a_{N-1}[X_{N-1}]\dots$

总结一下，动态规划的流程就是顺序产生所有的$m_t[x_t],a_t[x_t]$，再反向得到最大权重对应的路径。

## 8.4 粒子滤波（Particle Filtering）
对于变量可选择的值较多（比如需要预测温度到十分之一度）的情形，向前算法的时间复杂度就会较高。因此我们想通过贝叶斯网络中类似的采样技术来获得$P(X_N|e_{1:N})$。这种方法称之为粒子滤波。
实际上就是要通过采样的方法计算：
$$B(W_{i+1})\propto P(f_{i+1}|W_{i+1})\sum\limits_{w_i}P(W_{i+1}|w_i)B(w_i)$$
这个式子可以分为两部分：
时间流逝更新（Time Elapse Update），对应于$P(W_{i+1}|w_i)$;
观察更新（Observation Update），对应于$P(f_{i+1}|W_{i+1})$。
我们有n个样本，或者称为粒子，初始时随机分布在变量域d中，可以统计不同变量值出现的次数，得到初始的$B(W)$列表。
然后分别通过采样的方式实现时间流逝更新和观察更新。
时间流逝更新的做法是，对于每个粒子，按照转移模型的概率对该粒子的值进行更新，从而得到新的$B(W)$列表；
观察更新的方法类似于之前的似然加权。我们为每个粒子按照$P(f|W)$分配一个权重，然后再进行统计每个变量值出现的次数时需要带上此权重，得到新的$B(W)$列表。

粒子过滤的典型代码如下：
```python
class ParticleFilter(InferenceModule):
    def __init__(self, ghostAgent, numParticles=300):
        InferenceModule.__init__(self, ghostAgent)
        self.setNumParticles(numParticles)

    def setNumParticles(self, numParticles):
        self.numParticles = numParticles

    def initializeUniformly(self, gameState: busters.GameState):
        self.particles = []
        for i in range(self.numParticles):
            # 通过取模的方式，获得均匀分布在各个可能变量值上的粒子
            position = self.legalPositions[i % len(self.legalPositions)]
            self.particles.append(position)
    
    def getBeliefDistribution(self):
        # 根据各个粒子的值，得出各个值对应的概率列表
        belief = util.Counter()
        for particle in self.particles:
            belief[particle] += 1
        belief.normalize()
        return belief

    def observeUpdate(self, observation: int, gameState: busters.GameState):
        # 粒子过滤方法中的观察更新
        # 为不同粒子分配不同的权重,w = P(f|w)
        pacmanPosition = gameState.getPacmanPosition()
        jailPosition = self.getJailPosition()
        weights = util.Counter()

        for particle in self.particles:
            # 获取权重P(f_{i+1}|w_{i+1})
            prob = self.getObservationProb(observation , pacmanPosition , particle , jailPosition)
            weights[particle] += prob # 如果多个粒子都取到同一变量，需要把权重加起来

        # 所有粒子权重为零，需要重新初始化
        if sum(weights.values()) == 0:
            self.initializeUniformly(gameState)
        else:
            weights.normalize()
            # 根据粒子的权重，重新采样生成新的粒子列表
            new_particles = []
            for particle in self.particles:
                # 按照权重，取出新的粒子
                # util.sample(weights)返回的是weights的key，这里即是粒子
                new_particles.append(util.sample(weights))

            self.particles = new_particles

    def elapseTime(self, gameState):
        # 粒子过滤方法中的时间流逝更新
        # 对每个粒子，以一定的概率修改该粒子的值

        new_particles = []
        for particle in self.particles:
            # 获取更新的分布P(w_{i+1}|w_i)
            pos_dist = self.getPositionDistribution(gameState , particle)
            new_particle = pos_dist.sample()
            new_particles.append(new_particle)

        self.particles = new_particles
```

# 9 机器学习（Machine Learning，ML）
** 本节所讲述的机器学习比较浅显，可以当作对机器学习的初步认识**
之前学习了不确定条件下的推理模型，但其依赖的概率模型或者概率表是给定的。在有关机器学习的讨论中，将学习处理未给定该模型的情况。

机器学习分为两大类：监督和无监督。
监督学习算法通过推断输入数据与对应输出之间的关系，以预测新的输入数据的输出结果；
无监督学习只有输入没有输出，希望分组或处理这些输入数据。
之后将讨论的范畴限于监督学习。

通常数据集会分为三个部分：训练数据，验证数据和测试数据。验证数据相当于实用之前先测试模型的性能，如果不达标则需要重新训练。

## 9.1 朴素贝叶斯（Naives Bayes）
我们希望解决分类问题。即给定一些对象，希望为这些对象打上标签（少数的离散值）。训练时，我们给定对象和它们所属的标签，并希望从中学得某种关系，以应对新的对象的分类问题。
比如说我们要分类一些邮件，考察它们是有用还是没用的。邮件本身是字符串信息，为了从中提取信息，我们需要将其转化成一些特征（包含什么内容，大小写等一切可能的特征）。用特征函数f(x)来描述，其中x是直接的输入。
具体的，特征可以是是否包含某个单词。记第200个特征是包含单词“naive”，则如果该词出现在邮件中$f_200=1$，否则为0。

对于标签Y而言，我们可以计算概率：
$P(Y=good|f_1,f_2,\dots,f_n),P(Y=bad|f_1,f_2,\dots,f_n)$。并通过哪个值更高来对邮件进行标记。不过要想存下所有的概率，需要$2^{n+1}$的内存，显然是不够的。因此需要引入贝叶斯结构来优化存储。
我们假设其贝叶斯结构如下：
![alt text](/assets/images/image-23.png)
这说明在给定Y的情况下，所有特征之间条件独立。我们只需要存储n个$P(F_i|Y)$，每个只包含2*2=4个需要存储的概率。因此所需要的存储量是线性的。当然这个假设是很强的，实际上如果特征数目比较少，可能会把贝叶斯网络的结构变得更复杂一些（贝叶斯网络中增加边）。

依靠这些存储的变量，可以计算出我们希望的预测：
$prediction(f_1,\dots,f_n)=argmax_y(y|f_1,\dots,f_n)=argmax_y(y,f_1,\dots,f_n)=argmax_yP(y)\prod P(f_i|y)$
第二个等号用了链式法则，但是分母中的部分是已知的，因此概率是常数不影响我们的argmax操作。

现在我们已经知道了给定概率如何进行预测。接下来的问题是，如何从输入数据中得到CPT。

### 9.1.1 参数估计
根据n次采样我们得到了一个序列$x_1,x_2,\dots,x_n$，我们现在希望知道根据这个采样结果如何得到最有可能的$P(x_i)$。
首先我们要把每个$P(x_i)$设定为关于$\theta$的函数且要满足归一化。比如$P(x=1)=\theta,P(x=0)=1-\theta$（这样的分布假设称为伯努利分布）。
然后进行最大似然估计（MLE），即我们希望找到一个$\theta$，使得进行n次采样出现序列$x_1,x_2,\dots,x_n$的概率是最大的。方程表达即为：
$L(\theta)=P(x_1,x_2,\dots,x_n)=\prod P_{x_i}(\theta),\ \ \ \ \frac{\partial L}{\partial \theta} = 0$
求解以上方程可以得到$\theta$的值，代入设定好的$P(x_i)$关于$\theta$的函数，得到估计的概率分布。

### 9.1.2 朴素贝叶斯的最大似然估计
现在的问题是想要得到概率表P(F|Y)。这个概率的逻辑就是，在已知邮件好坏的情况下，出现某个词语i的概率。得到这个概率的想法是很自然的，通过我们的数据集，在好的邮件中统计一下有多少封包含词语i即可。
不过这个方法不是那么严谨，对于每个F的变量值个数超过2的情况并不好推广。我们可以按照如上最大似然的方式进行推导验证：
$L(\theta)=\prod \theta^{f_i}(1-\theta)^{1-f_i},i$脚标表示的是第i个数据点；
直接求导苦难，可以先取对数：$\log L(\theta)=\log\theta \sum f_i + \log(1-\theta)\sum(1-f_i)$，
再进行求导：$\frac{1}{\theta}\sum f_i = \frac{1}{1-\theta}\sum(1-f_i)$,
于是有$\theta = \frac{\sum f_i}{\sum 1}=\frac{f}{N}$。和我们之前直接计算的概率相同。

### 9.1.3 平滑性（Smoothing）
如果数据集比较差，比如恰好没有在好邮件中出现某个词i，那么根据以上最大似然得到的概率估计，会在测试中直接把看到词语i的归入坏邮件中（概率为0）。显然这个判断太武断，我们用一个很小的概率来代替他，已达到平滑效果。所使用的方法加做拉普拉斯估计：
$P_{LAP,k}(x)=\frac{count(x)+k}{N+k|X|}$；
$P_{LAP,k}(x|y)=\frac{count(x,y)+k}{count(y)+k|X|}$；
其中|X|表示变量X可选择的值的个数。k的值需要试错得出，k为0的时候回归到最大似然估计；k很大的时候变量X的概率分布是均等的。

## 9.2 感知机（Perceptron）
### 9.2.1 线性分类器
对于一个对象，我们可以提取出其特征，并得到特征向量f。一个简单的分类方法是，我们给定某一类对应的特征向量的范围，只要特征向量在该范围内，即可打上标签。
以邮件分类的二元分类为例，通过训练数据，我们可以得到好邮件的标志特征向量w。我们只需检测$f\cdot w$的正负，即可判断特征向量是在标准向量的同侧还是异侧，从而确定邮件的好坏。
用数学语言来表示，就是计算激活函数：$activation_w(x)=w^Tf(x)$，并通过其符号进行分类。

### 9.2.2 二元感知机
现在我们考虑如何得到w。
标签类别y可以取值$\pm 1$，+1对应于特征向量f与w点积为正。从训练数据上得到w的方式是：
（1）w初始为0向量；
（2）对于每个训练数据点，用当前的w检测一遍；
（3）如果分类正确，则不进行任何修改；
（4）如果分类不正确，则更新w：w'=w+yf(x)，这里y是数据点的真实标签；
我们可以这么理解这个更新。我们自然希望w'f得到的结果比wf更好，我们发现差值是$y|f|^2$。这保证了我们把校验值向正确的y方向进行了最大程度的移动$|f|^2$。
典型的代码如下所示：
```python
finish = False
while not finish:
    finish = True
    for batch in dataloader:
        x = batch['x']
        y = batch['label']
        pred = model.get_prediction(x)
        if pred.item() != y.item():
            model.w += y * x # 更新权重
            finish = False
```

### 9.2.3 偏置项
对如上问题，有一点让人很疑惑。就是零向量，比如说所有特征词都不出现的邮件到底是好还是不好。我们似乎始终无法判断，因为分界面始终穿过原点。
为了使得零向量有明确分类，或者说得到其他的分界面，我们可以这么做：为所有特征向量增加一个维度，这个维度上的值始终为1，不过初始化w时需要在对应维度上赋值b。
仍然按照上述方法进行更新w，实际上我们检测的是$w\cdot f+b$这个量的正负，即分界面变成了$w\cdot f+b=0$。（这里的w和f都是之前的w和f，并不是添加了一个维度的）

### 9.2.4 多类感知机
对于多个标签的分类，也可以使用如上方法。只不过需要为每一个类维护一个权重向量w。根据w与f的点积最大值所对应的标签类作为实际的标签。
此时的更新发生了一些变化：
（1）w初始为0向量；
（2）对于每个训练数据点，用当前的w检测一遍；
（3）如果分类正确，则不进行任何修改；
（4）如果分类不正确，真实分类y，预测分类y'，需要：$w_y += f(x),w_{y'} -= f(x)$。
这样的更新很好理解，即惩罚错误分类，补偿到正确的分类上去。如果要添加偏置，也只需要增加一个维度，在特征向量上该维度始终为1，在w上设定为某个值即可。

## 9.3 回归（Regression）
### 9.3.1 线性回归（Linear Regression）
给定特征向量**x**，我们想通过函数$h(\mathbf{x})=\mathbf{w^T x}$来预测特征向量x对应的标签。现在的问题是如何得到**w**？
我们的训练数据可以当作是N个特征向量**x**，这些向量可以组成一个矩阵**X**，这个**X**矩阵的每一行是一个特征向量**x**。
把这N个数据的标签记为列向量**y**，于是乎我们的目的就是要求解向量**w**，使得预测标签**Xw**（一个列向量，每个元素对应一个数据点的预测标签）和**y**向量充分接近。这个问题可以通过最小二乘法解决：
确定损失函数$Loss(\mathbf{w})=\frac{1}{2}||\mathbf{y-Xw}||^2$，对**w**求梯度有：
$\mathbf{0=\frac{1}{2}\nabla_w(y-Xw)^T(y-Xw)}$
$\mathbf{=\frac{1}{2}\nabla_w(y^Ty-y^TXw-w^TX^Ty+w^TX^TXw)}$
$\mathbf{=-X^Ty+X^TXw}$
于是可以得到:
$$\mathbf{w=(X^TX)^{-1}{X^Ty}}$$

### 9.3.2 梯度下降（Gradient Descent）
上一节中，最后需要求逆得到**w**，但是矩阵求逆操作是昂贵的，我们需要替换得到**w**的方式。一种常用方法是梯度下降法，即为了最小化函数$h(\mathbf{w})$，可以按照$\mathbf{w\to w-\alpha \cdot \nabla_w}h(\mathbf{w})$迭代更新，其中$\alpha$为学习率。
在我们的最小二乘法模型中，即为：
$$\mathbf{w\to w -\alpha \cdot (-X^Ty+X^TXw)}$$

### 9.3.3 逻辑回归（Logistic Regression）
之前我们通过$h(\mathbf{x})=\mathbf{w^T x}$的值来直接得到标签，现在我们通过如下函数（sigmoid函数）来描述特定标签的概率：
$h(\mathbf{x})=\frac{1}{1+e^-{\mathbf{w^T x}}}$，这个函数值被限定在0和1之间。
对于二元问题，如果概率超过0.5则认定属于标签1，否则属于标签0。同样可以通过训练数据得到最小化损失函数$Loss(\mathbf{w)=\frac{1}{2}||y-h_w(x)||^2}$的向量**w**:
$grad = \mathbf{\nabla_w}Loss(\mathbf{w)=-(y-h_w(x))h_w(x)(1-h_w(x))x}$

### 9.3.4 多类逻辑回归
上面的函数可以帮我们把一个特征向量映射到二元问题的概率上。如果有多个可选择的标签，需使用softmax函数来替代逻辑函数（sigmoid函数）：
$$P(y=i|f,W)=\frac{e^{\mathbf{w}_i^T\mathbf{f(x)}}}{\sum\limits_k e^{\mathbf{w}_k^T\mathbf{f(x)}}}$$
这个数指示了当前特征向量f属于标签i的概率，它是归一化的（属于不同标签的概率之和为1）。
对于n个数据点(x,y)，我们的目标是最大化联合概率，即每个数据点预测正确的总概率：
$$l(W)=\prod\limits_i P(y_i|f(x_i),W)$$
想要获得最大化函数l(W)的矩阵W，需要使用梯度上升法。
同样，我们取对数，即最大化$\log l(W)$，它的梯度为：
$$grad_{w_j}=\sum\limits_i(\delta_{y_i,j}-\frac{e^{w_j^Tf(x_i)}}{\sum\limits_k e^{w_k^Tf(x_i)}})f(x_i)$$

## 9.4 神经网络（Neural Networks）
### 9.4.1 非线性分类
考虑二元分类任务，现在我们只有一个特征向量，这相当于要把一维空间分成两个部分，边界是零维的。但是对于有些数据集，仅靠这一个特征是无法完成划分的：
![alt text](/assets/images/image-24.png)
上图中，我们无法找到一个点把蓝色点和红色点划分到不同区域。通常为了实现划分，我们需要引入第二个特征。在二维平面中边界是一条线，就有可能把数据集中不同颜色的点完全分到不同的区域之中：
![alt text](/assets/images/image-25.png)
这里我们两个个特征函数选择的是$x_1=x,x_2=x^2$。
通过手动添加特征能够帮助进行分类，但是不具有通用性。我们的目标是找到通用的解决方法。究其原因，在于我们之前的分类界限总是一条直线或是平面，而我们希望通过一种模型（用w刻画），能够学到很复杂的边界。

### 9.4.2 多层感知机
![alt text](/assets/images/image-26.png)
考察一个两层的感知机。我们有三个特征向量，先按照三种不同的权重求和，可以知道三个数。对于这三个数，我们作用一个简单的函数（这里是sgn函数），得到三个函数值。把这三个函数值再按照一组权重加起来，再用sgn函数判断一下，就能得到一个最终的决断。
考虑原先的模型，我们只进行一次加权求和并使用sgn函数判断，其边界在原先的特征空间中总是一个平面。不同的权重w只是代表了不同的平面。而对于这种多层的结构，我们最终进行的判断的分界点在原先的特征向量空间中对应的可能就是一个非线性的复杂的分界面，而这个复杂的分界面，就由其中的12个权重值共同维护。
![alt text](/assets/images/image-27.png)
我们可以继续加深中间的网络。理论上，通过增加深度和神经元的数量（中间进行的函数），我们可以通过合适的权重值，使得最后的边界在原先的特征空间中逼近任意的函数。
从感觉上这是好理解的，我们可以很轻易地增加变量，而更多的变量显然就意味着更大的信息量，理论上就能够表达出更复杂的函数。

### 9.4.3 损失函数
**与之前的二元分类和多类标签完全类似**
二元感知的准确度可以用如下方法表述：
$l(w)=\frac{1}{n}\sum[sgn(w\cdot f(x_i))==y_i]$，
多元感知的准确度可以用如下方式表述：
$l(w)=\prod\limits_i P(y_i=j|f(x_i),w)=\prod \frac{e^{f(x_i)^Tw_j}}{\sum\limits_k e^{w_k^Tf(x_i)}}$
我们想要做的即最大化正确率，可以通过梯度上升的方法得到：
$\nabla_w l(W)=[\frac{\partial l(w)}{\partial w_1} ,\frac{\partial l(w)}{\partial w_2},\dots \frac{\partial l(w)}{\partial w_n}]$
在神经网络优化中，由于数据集很大，因此常用批量梯度上升的方法（Batch Gradient Ascent）进行计算。

### 9.4.4 函数选择
在神经网络的中间层中，我们需要施加一些函数。在之前的例子中，我们用的是阶跃函数sgn，由于其不连续，因此不便于后续求梯度。可以用下面这些连续的函数进行代替：
**Sigmoid函数**:$\sigma(x)=\frac{1}{1+e^{-x}}$
![alt text](/assets/images/image-28.png)
**ReLU函数**：$f(x)=\begin{cases}0,\ if \ \ x<0\\
x,\ id \ \ x\ge 0\end{cases}$
![alt text](/assets/images/image-29.png)

# 10 逻辑推理
本节主要讲述逻辑语言与推断相关的理论知识，不太重要，可以跳过。
## 10.1 逻辑语言
$\neg $，非，$\neg P$为真仅当P为假命题；
$\wedge$，与，$A\wedge B$为真仅当A和B都为真；
$\vee$，或，$A\vee B$为真仅当A或B为真；
$\Rightarrow$，推出，$A\Rightarrow B$ 为假仅当A为真B为假。这里可以这么理解，$A\Rightarrow B$值的是，如果有A就有B，那么当A不成立时，这个论断仍然是正确的；
$\Leftrightarrow$，等价，$A\Leftrightarrow B$为真仅当A和B同为真或同为假。

## 10.2 语句逻辑
命题A和B，各自可以为真假，共有4中情况。
对于一个命题，有如下可能：
有效，对所有情况都为真，比如$\neg A \vee A$；
可满足，对部分情况为真，比如$A\vee B$；
不可满足，对所有情况都为假，比如$\neg A \wedge A$。

复杂的命题可以化简，有如下化简公式：
![alt text](/assets/images/image-30.png)

利用化简公式可以将逻辑语句变形。一种标准的语句形式称为合取形式（CNF），它是或的与：$(P_1\vee \dots \vee P_i)\wedge\dots\wedge(P_j\vee \dots \vee P_n)$。CNF语句可以通过与简单的组合在一起。

任何语句都可写成CNF语句，考虑如下例子$A\Leftrightarrow (B\vee C)$的转换：
（1）展开等价符号，$(A\Rightarrow(B\vee C))\wedge((B\vee C)\Rightarrow A)$；
（2）替换推出符号，并使用结合律去括号：$(\neg A\vee B\vee C)\wedge (\neg(B \vee C)\vee A)$；
（3）将非符号进行去括号，使其作用在单一命题上：$(\neg A\vee B\vee C)\wedge (\neg B \wedge C)\vee A$
（4）使用分配律：$(\neg A\vee B\vee C)\wedge (\neg B \vee A)\wedge   (\neg C \vee A)$

## 10.3 命题逻辑推理
蕴含关系，$A \models B$说明所有A为真的情况下B也为真。我们通常需要做的就是判断在一个逻辑库中KB中是否包含一个命题p。
证明蕴含关系有两种方法：
（1）$A\Rightarrow B$能够证明$A \models B$；
（2）$A \wedge \neg B$能够证伪$A \models B$；

通常而言，要证明一个逻辑库KB中是否包含某个命题q，需要进行枚举所有KB为真的情况，检查命题q是否为真。其时间复杂度是指数的，这是co-NP完全的，最差情况必然是指数的。但在算法实践中可以更快地终止。下面介绍两种方法。

### 10.3.1 DPLL算法
DPLL算法接受一个CNF句子，并判断其是否是可满足的。
其大体结构为深度优先搜索然后进行回溯，即按照变量顺序（即最小的命题单位A，B……）进行赋值，每一次枚举要先完整检测完是否可满足再回溯进行赋当前变量的下一个值。
在回溯搜索的过程中，有三个可以化简的部分。每次将要新赋值一个变量时都要先进行如下三步：
（1）是否提前终止。CNF是一堆或的与的形式，如果所有的分句中有一个为假，则整体的与必然为假，无需后续的赋值判断；如果某一个分句内有一个为真，则该分句也必然为真，无需进行该分句的后续赋值；
（2）是否可以纯符号化简。将某个符合（变量）仅以肯定或者否定形式出现时，可以直接将其赋值为真或假（因为另一种情况对分句的或无贡献），并进行语句的化简；
（3）是否可以单元子句化简。一个子句如果仅由一个符号或者是一个符号和多个符号的或组成，则可以立即为其赋值（只有一种情况，否则整体的与无法满足）。

### 10.3.2 向前链接算法（Forward Checking）
向前算法的核心是，从已知事实出发，利用知识库中的规则，不断推导出所有能够推导出的结。如果在推导出的结论中包含了查询的q，即可说明知识库KB中蕴含q。
注意这种方法只能被用于知识库仅由确定语句构成的情况，确定语句指的是如下形式的句子：
$P_1\wedge P_2\wedge\dots \wedge P_n \Rightarrow Q = \neg P_1 \vee \neg P_2 \vee \dots \vee \neg P_n \vee Q$
一个具体的例子如下所示：
![alt text](/assets/images/image-31.png)
维护如下三个数据结构：
count[c]，为知识库中的某一条规则c记录其中的前提部分（$\Rightarrow$以前）尚未被确定的符号数量；
inferred[s]，记录符号s是否被推断出来并处理过了，初始所有都为False；
agenda，队列，存储已经推断为真，但是尚未被处理（由此进行推断）的符号；

程序的过程如下：
（1）agenda非空，则取出一个符号p进行处理；
（2）如果p恰好是目标，则任务完成；
（3）如果p未被处理，则inferred标记已处理，并开始处理；
（4）对于规则库中所有在前提中含有p的规则c进行更新；
（5）count[c] -= 1；
（6）如果count[c] == 0，说明该规则被激活，将c的结果（$\Rightarrow$以后）符号加入agenda；
（7）如果agenda已空，仍然找不到q，则返回失败。

## 10.4 一阶逻辑（First Order Logic）
一阶逻辑是面向对象的逻辑表达式，其严谨定义难以表述，我们可以通过如下例子理解：
$\forall x , \exists y , SoulMate(x,y)$
这句话的意思是，任何人都存在是他的灵魂伴侣的人。其中的x，y指代某一个对象；SoulMate()是个函数，其输入和输出也都是对象。
FOL中的等号表示指代同一符号：
$Wife(Einstein)=FirtstCounsin(Einstein)$
表示Einstein的妻子和第一个表亲是同一个人（对象）。

一阶逻辑的语法中有存在和任意，一些与之相关的公式如下：
![alt text](/assets/images/image-32.png)

在FOL表述下，同样可以判断知识库KB中是否蕴含命题q。比如说可以进行如下推理：
$(\forall x A(x)\wedge B(x)) \Rightarrow C(x) \wedge A(David) \wedge B(David)$
我们可以按照逻辑推出C(David)。

## 10.5 逻辑智能体
用我们以上所述的逻辑表述方式，能够进行逻辑推理。当把世界的规则用正确的逻辑形式告诉智能体之后，我们希望其能根据这些规则自行推理。
通常而言，这些规则中包含时间这一维度。简要说明一下如何将其写入逻辑语言规则中。比如说我们想告诉智能体：到达岩浆附近空气会变热，而空气变热说明有危险，需要远离。
写成逻辑语言：
$Hot_{t+1} \Leftrightarrow (Forward_t \vee (Hot_t \wedge \neg Backward_t))$
即t+1时刻空气热的原因是t时刻向前了一步或者t时刻空气热但是没有远离。